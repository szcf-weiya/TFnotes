{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Keras for Researchers\n",
    "https://keras.io/getting_started/intro_to_keras_for_researchers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(3.0)\n",
    "with tf.GradientTape() as tape:\n",
    "    y = x**2\n",
    "    \n",
    "dy_dx = tape.gradient(y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=6.0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dy_dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.3812343,  1.9505316]], dtype=float32)>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = tf.Variable(tf.random.normal((3, 2)), name = 'w')\n",
    "b = tf.Variable(tf.zeros(2, dtype=tf.float32), name = 'b')\n",
    "x = [[1., 2., 3.]]\n",
    "x @ w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[ 4.2237244, -0.8030729]], dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x @ w + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'b:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://alysivji.github.io/python-matrix-multiplication-operator.html\n",
    "> PEP 465 introduced the @ infix operator that is designated to be used for matrix multiplication. The acceptance and implementation of this proposal in Python 3.5 was a signal to the scientific community that Python is taking its role as a numerical computation language very seriously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    y = x @ w + b\n",
    "    loss = tf.reduce_mean(y**2)\n",
    "    \n",
    "[dl_dw, dl_db] = tape.gradient(loss, [w, b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where `persistent` \n",
    "> Boolean controlling whether a persistent gradient tape is created. False by default, which means at most one call can be made to the gradient() method on this object.\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/GradientTape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "?tf.reduce_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "?tf.GradientTape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
       "array([[-0.3812343,  1.9505316],\n",
       "       [-0.7624686,  3.9010632],\n",
       "       [-1.1437029,  5.851595 ]], dtype=float32)>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl_dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([-0.3812343,  1.9505316], dtype=float32)>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_vars = {\n",
    "    'w': tf.Variable(tf.random.normal((3, 2)), name='w'),\n",
    "    'b': tf.Variable(tf.zeros(2, dtype=tf.float32), name='b')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad = tape.gradient(loss, my_vars)\n",
    "grad['b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'w': <tf.Variable 'w:0' shape=(3, 2) dtype=float32, numpy=\n",
       " array([[-0.5689192 , -0.5657398 ],\n",
       "        [-0.3709449 , -0.3143288 ],\n",
       "        [-0.158459  ,  0.12605582]], dtype=float32)>,\n",
       " 'b': <tf.Variable 'b:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)>}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'w': None, 'b': None}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad # the variables have been changed!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'w': None, 'b': None}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    y = x @ w + b\n",
    "    loss = tf.reduce_mean(y**2)\n",
    "grad = tape.gradient(loss, my_vars)\n",
    "grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'w': <tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
       " array([[-1.7861859, -0.81623  ],\n",
       "        [-3.5723717, -1.63246  ],\n",
       "        [-5.3585577, -2.44869  ]], dtype=float32)>,\n",
       " 'b': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([-1.7861859, -0.81623  ], dtype=float32)>}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    y = x @ my_vars['w'] + my_vars['b']\n",
    "    loss = tf.reduce_mean(y**2)\n",
    "grad = tape.gradient(loss, my_vars)\n",
    "grad['b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([-0.3812343,  1.9505316], dtype=float32)>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_vars = {\n",
    "    'w': w,\n",
    "    'b': b\n",
    "}\n",
    "grad = tape.gradient(loss, my_vars)\n",
    "grad['b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'w': None, 'b': None}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_vars = {\n",
    "    'w': tf.Variable(tf.random.normal((3, 2)), name='w'),\n",
    "    'b': tf.Variable(tf.zeros(2, dtype=tf.float32), name='b')\n",
    "}\n",
    "grad = tape.gradient(loss, my_vars)\n",
    "grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "?tf.keras.layers.Dense \n",
    "# or more details: https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = tf.keras.layers.Dense(2, activation = 'relu')\n",
    "x = tf.constant([[1., 2., 3.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.7364416, 1.2341949]], dtype=float32)>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    y = layer(x)\n",
    "    loss = tf.reduce_mean(y**2)\n",
    "\n",
    "grad = tape.gradient(loss, layer.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
       " array([[0.7364416, 1.2341949],\n",
       "        [1.4728832, 2.4683897],\n",
       "        [2.2093248, 3.7025847]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.7364416, 1.2341949], dtype=float32)>]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense_1/kernel:0, shape: (3, 2)\n",
      "dense_1/bias:0, shape: (2,)\n"
     ]
    }
   ],
   "source": [
    "for var, g in zip(layer.trainable_variables, grad):\n",
    "    print(f'{var.name}, shape: {g.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "?zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=1.0>, None]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.constant(1.0)\n",
    "v0 = tf.Variable(2.0)\n",
    "v1 = tf.Variable(2.0)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    tape.watch(x)\n",
    "    if x > 0.0:\n",
    "        result = v0\n",
    "    else:\n",
    "        result = v1**2\n",
    "        \n",
    "tape.gradient(result, [v0, v1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the control statement itself is not differentiable\n",
    "tape.gradient(result, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResourceVariable : tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "ResourceVariable : tf.Tensor(1.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(2.0)\n",
    "for epoch in range(2):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y = x + 1\n",
    "    \n",
    "    print(type(x).__name__, \":\", tape.gradient(y, x))\n",
    "    x.assign_add(1) # NOT x = x + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable([[1.0, 2.0],\n",
    "                 [3.0, 4.0]], dtype=tf.float32)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  x2 = x**2\n",
    "\n",
    "  # This step is calculated with NumPy\n",
    "  y = np.mean(x2, axis=0)\n",
    "\n",
    "  # Like most ops, reduce_mean will cast the NumPy array to a constant tensor using `tf.convert_to_tensor` \n",
    "  y = tf.reduce_mean(y, axis=0)\n",
    "\n",
    "print(tape.gradient(y, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "?tf.convert_to_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The dtype of the target tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.int32\n",
      "WARNING:tensorflow:The dtype of the source tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.int32\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# The x0 variable has an `int` dtype.\n",
    "x = tf.Variable([[2, 2],\n",
    "                 [2, 2]])\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  # The path to x1 is blocked by the `int` dtype here.\n",
    "  y = tf.cast(x, tf.float32)\n",
    "  y = tf.reduce_sum(x)\n",
    "\n",
    "print(tape.gradient(y, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "x0 = tf.Variable(3.0)\n",
    "x1 = tf.Variable(0.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  # Update x1 = x1 + x0.\n",
    "  x1.assign_add(x0)\n",
    "  # The tape starts recording from x1.\n",
    "  y = x1**2   # y = (x1 + x0)**2\n",
    "\n",
    "# This doesn't work.\n",
    "print(tape.gradient(y, x0))   #dy/dx0 = 2*(x1 + x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable([2., 2.])\n",
    "y = tf.Variable(3.)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  z = y**2\n",
    "# print(tape.gradient(z, x, unconnected_gradients=tf.UnconnectedGradients.ZERO))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(tape.gradient(z, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://keras.io/getting_started/intro_to_keras_for_researchers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, units=32, input_dim=32):\n",
    "        super(Linear, self).__init__()\n",
    "        w_init = tf.random_normal_initializer()\n",
    "        self.w = tf.Variable(\n",
    "            initial_value = w_init(shape=(input_dim, units), dtype = \"float32\"),\n",
    "            trainable = True\n",
    "        )\n",
    "        b_init = tf.zeros_initializer()\n",
    "        self.b = tf.Variable(\n",
    "            initial_value = b_init(shape=(units,), dtype = \"float32\"),\n",
    "            trainable = True\n",
    "        )\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_layer = Linear(units = 4, input_dim=2)\n",
    "y = linear_layer(tf.ones((2, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
       "array([[ 0.00323859,  0.02699355, -0.07890009, -0.03770854],\n",
       "       [ 0.00323859,  0.02699355, -0.07890009, -0.03770854]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'Variable:0' shape=(2, 4) dtype=float32, numpy=\n",
       " array([[ 0.03596107, -0.01020826, -0.02216507, -0.04651199],\n",
       "        [-0.03272248,  0.03720182, -0.05673502,  0.00880345]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'Variable:0' shape=(4,) dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_layer.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(keras.layers.Layer):\n",
    "    def __init__(self, units = 32):\n",
    "        super(Linear, self).__init__()\n",
    "        self.units = units\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(\n",
    "            shape = (input_shape[-1], self.units),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            shape = (self.units,),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True\n",
    "        )\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_layer = Linear(4)\n",
    "y = linear_layer(tf.ones((2,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train.reshape(60000, 784).astype(\"float32\") / 255, y_train)\n",
    ")\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((None, 784), (None,)), types: (tf.float32, tf.uint8)>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_layer = Linear(10)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 , loss =  1.6134514808654785\n",
      "step 1 , loss =  1.674712896347046\n",
      "step 2 , loss =  1.699915885925293\n",
      "step 3 , loss =  1.7053558826446533\n",
      "step 4 , loss =  1.6796443462371826\n",
      "step 5 , loss =  1.609327793121338\n",
      "step 6 , loss =  1.6452875137329102\n",
      "step 7 , loss =  1.747910499572754\n",
      "step 8 , loss =  1.713761329650879\n",
      "step 9 , loss =  1.705289363861084\n",
      "step 10 , loss =  1.6537827253341675\n",
      "step 11 , loss =  1.6228864192962646\n",
      "step 12 , loss =  1.7047626972198486\n",
      "step 13 , loss =  1.6979721784591675\n",
      "step 14 , loss =  1.7081371545791626\n",
      "step 15 , loss =  1.676334261894226\n",
      "step 16 , loss =  1.5624463558197021\n",
      "step 17 , loss =  1.6063860654830933\n",
      "step 18 , loss =  1.6761558055877686\n",
      "step 19 , loss =  1.6598820686340332\n",
      "step 20 , loss =  1.5930603742599487\n",
      "step 21 , loss =  1.643523931503296\n",
      "step 22 , loss =  1.6386659145355225\n",
      "step 23 , loss =  1.6144518852233887\n",
      "step 24 , loss =  1.5751562118530273\n",
      "step 25 , loss =  1.6180683374404907\n",
      "step 26 , loss =  1.5564169883728027\n",
      "step 27 , loss =  1.661069393157959\n",
      "step 28 , loss =  1.6290698051452637\n",
      "step 29 , loss =  1.6606338024139404\n",
      "step 30 , loss =  1.5904649496078491\n",
      "step 31 , loss =  1.6499195098876953\n",
      "step 32 , loss =  1.6027448177337646\n",
      "step 33 , loss =  1.6386260986328125\n",
      "step 34 , loss =  1.6069972515106201\n",
      "step 35 , loss =  1.5404870510101318\n",
      "step 36 , loss =  1.6963770389556885\n",
      "step 37 , loss =  1.5334930419921875\n",
      "step 38 , loss =  1.6415176391601562\n",
      "step 39 , loss =  1.65771484375\n",
      "step 40 , loss =  1.6126055717468262\n",
      "step 41 , loss =  1.6678681373596191\n",
      "step 42 , loss =  1.6844251155853271\n",
      "step 43 , loss =  1.6062049865722656\n",
      "step 44 , loss =  1.565512776374817\n",
      "step 45 , loss =  1.600064992904663\n",
      "step 46 , loss =  1.635268211364746\n",
      "step 47 , loss =  1.596848964691162\n",
      "step 48 , loss =  1.67972731590271\n",
      "step 49 , loss =  1.6184649467468262\n",
      "step 50 , loss =  1.6670693159103394\n",
      "step 51 , loss =  1.6055474281311035\n",
      "step 52 , loss =  1.6000863313674927\n",
      "step 53 , loss =  1.6454558372497559\n",
      "step 54 , loss =  1.6057820320129395\n",
      "step 55 , loss =  1.5332688093185425\n",
      "step 56 , loss =  1.5783770084381104\n",
      "step 57 , loss =  1.4902396202087402\n",
      "step 58 , loss =  1.6848124265670776\n",
      "step 59 , loss =  1.659125566482544\n",
      "step 60 , loss =  1.5570895671844482\n",
      "step 61 , loss =  1.6167190074920654\n",
      "step 62 , loss =  1.6540415287017822\n",
      "step 63 , loss =  1.6854040622711182\n",
      "step 64 , loss =  1.5994794368743896\n",
      "step 65 , loss =  1.648134469985962\n",
      "step 66 , loss =  1.6231489181518555\n",
      "step 67 , loss =  1.528334140777588\n",
      "step 68 , loss =  1.5042541027069092\n",
      "step 69 , loss =  1.588270664215088\n",
      "step 70 , loss =  1.6143748760223389\n",
      "step 71 , loss =  1.5477361679077148\n",
      "step 72 , loss =  1.5901784896850586\n",
      "step 73 , loss =  1.5416572093963623\n",
      "step 74 , loss =  1.5031874179840088\n",
      "step 75 , loss =  1.5317076444625854\n",
      "step 76 , loss =  1.5278302431106567\n",
      "step 77 , loss =  1.5812482833862305\n",
      "step 78 , loss =  1.544041633605957\n",
      "step 79 , loss =  1.591056227684021\n",
      "step 80 , loss =  1.5884935855865479\n",
      "step 81 , loss =  1.5653998851776123\n",
      "step 82 , loss =  1.5959354639053345\n",
      "step 83 , loss =  1.5569288730621338\n",
      "step 84 , loss =  1.5478146076202393\n",
      "step 85 , loss =  1.5200726985931396\n",
      "step 86 , loss =  1.5578906536102295\n",
      "step 87 , loss =  1.5652726888656616\n",
      "step 88 , loss =  1.5114314556121826\n",
      "step 89 , loss =  1.4787583351135254\n",
      "step 90 , loss =  1.6731948852539062\n",
      "step 91 , loss =  1.5356314182281494\n",
      "step 92 , loss =  1.5820858478546143\n",
      "step 93 , loss =  1.6542162895202637\n",
      "step 94 , loss =  1.6033885478973389\n",
      "step 95 , loss =  1.5844755172729492\n",
      "step 96 , loss =  1.5837247371673584\n",
      "step 97 , loss =  1.6159875392913818\n",
      "step 98 , loss =  1.678106665611267\n",
      "step 99 , loss =  1.6516976356506348\n",
      "step 100 , loss =  1.6553864479064941\n",
      "step 101 , loss =  1.566811203956604\n",
      "step 102 , loss =  1.6255779266357422\n",
      "step 103 , loss =  1.5505495071411133\n",
      "step 104 , loss =  1.5677990913391113\n",
      "step 105 , loss =  1.6362714767456055\n",
      "step 106 , loss =  1.5062055587768555\n",
      "step 107 , loss =  1.603452444076538\n",
      "step 108 , loss =  1.6077253818511963\n",
      "step 109 , loss =  1.5597130060195923\n",
      "step 110 , loss =  1.5728583335876465\n",
      "step 111 , loss =  1.608298659324646\n",
      "step 112 , loss =  1.6557406187057495\n",
      "step 113 , loss =  1.716658115386963\n",
      "step 114 , loss =  1.6363725662231445\n",
      "step 115 , loss =  1.5612506866455078\n",
      "step 116 , loss =  1.6177080869674683\n",
      "step 117 , loss =  1.6301082372665405\n",
      "step 118 , loss =  1.6322052478790283\n",
      "step 119 , loss =  1.5859466791152954\n",
      "step 120 , loss =  1.5522949695587158\n",
      "step 121 , loss =  1.5647988319396973\n",
      "step 122 , loss =  1.527848720550537\n",
      "step 123 , loss =  1.5992072820663452\n",
      "step 124 , loss =  1.5661429166793823\n",
      "step 125 , loss =  1.6080219745635986\n",
      "step 126 , loss =  1.5613479614257812\n",
      "step 127 , loss =  1.5614285469055176\n",
      "step 128 , loss =  1.6380910873413086\n",
      "step 129 , loss =  1.6905953884124756\n",
      "step 130 , loss =  1.5926748514175415\n",
      "step 131 , loss =  1.574023962020874\n",
      "step 132 , loss =  1.4946423768997192\n",
      "step 133 , loss =  1.5623154640197754\n",
      "step 134 , loss =  1.5759568214416504\n",
      "step 135 , loss =  1.5482540130615234\n",
      "step 136 , loss =  1.5781779289245605\n",
      "step 137 , loss =  1.6023428440093994\n",
      "step 138 , loss =  1.4371455907821655\n",
      "step 139 , loss =  1.4594006538391113\n",
      "step 140 , loss =  1.579574465751648\n",
      "step 141 , loss =  1.503282070159912\n",
      "step 142 , loss =  1.580687165260315\n",
      "step 143 , loss =  1.4117119312286377\n",
      "step 144 , loss =  1.5954536199569702\n",
      "step 145 , loss =  1.5349888801574707\n",
      "step 146 , loss =  1.575685977935791\n",
      "step 147 , loss =  1.574859857559204\n",
      "step 148 , loss =  1.5659711360931396\n",
      "step 149 , loss =  1.5503175258636475\n",
      "step 150 , loss =  1.4855303764343262\n",
      "step 151 , loss =  1.5456559658050537\n",
      "step 152 , loss =  1.5529639720916748\n",
      "step 153 , loss =  1.666565179824829\n",
      "step 154 , loss =  1.4897702932357788\n",
      "step 155 , loss =  1.5501244068145752\n",
      "step 156 , loss =  1.4867808818817139\n",
      "step 157 , loss =  1.4928123950958252\n",
      "step 158 , loss =  1.5749199390411377\n",
      "step 159 , loss =  1.4397714138031006\n",
      "step 160 , loss =  1.5603418350219727\n",
      "step 161 , loss =  1.5078846216201782\n",
      "step 162 , loss =  1.5398297309875488\n",
      "step 163 , loss =  1.5491681098937988\n",
      "step 164 , loss =  1.5035300254821777\n",
      "step 165 , loss =  1.4848012924194336\n",
      "step 166 , loss =  1.4462326765060425\n",
      "step 167 , loss =  1.553803563117981\n",
      "step 168 , loss =  1.545175552368164\n",
      "step 169 , loss =  1.5469963550567627\n",
      "step 170 , loss =  1.531288743019104\n",
      "step 171 , loss =  1.5490273237228394\n",
      "step 172 , loss =  1.635106086730957\n",
      "step 173 , loss =  1.533498764038086\n",
      "step 174 , loss =  1.477848768234253\n",
      "step 175 , loss =  1.5644147396087646\n",
      "step 176 , loss =  1.6075725555419922\n",
      "step 177 , loss =  1.50184965133667\n",
      "step 178 , loss =  1.5504810810089111\n",
      "step 179 , loss =  1.6192353963851929\n",
      "step 180 , loss =  1.5876141786575317\n",
      "step 181 , loss =  1.526900291442871\n",
      "step 182 , loss =  1.5562052726745605\n",
      "step 183 , loss =  1.5372216701507568\n",
      "step 184 , loss =  1.6100876331329346\n",
      "step 185 , loss =  1.5698639154434204\n",
      "step 186 , loss =  1.4614732265472412\n",
      "step 187 , loss =  1.543630599975586\n",
      "step 188 , loss =  1.5730323791503906\n",
      "step 189 , loss =  1.6088868379592896\n",
      "step 190 , loss =  1.545328140258789\n",
      "step 191 , loss =  1.5974178314208984\n",
      "step 192 , loss =  1.4192283153533936\n",
      "step 193 , loss =  1.4903268814086914\n",
      "step 194 , loss =  1.5478326082229614\n",
      "step 195 , loss =  1.5110808610916138\n",
      "step 196 , loss =  1.6362098455429077\n",
      "step 197 , loss =  1.4304749965667725\n",
      "step 198 , loss =  1.441995620727539\n",
      "step 199 , loss =  1.6014695167541504\n",
      "step 200 , loss =  1.5747066736221313\n",
      "step 201 , loss =  1.5380640029907227\n",
      "step 202 , loss =  1.6206812858581543\n",
      "step 203 , loss =  1.657726526260376\n",
      "step 204 , loss =  1.7751851081848145\n",
      "step 205 , loss =  1.4905415773391724\n",
      "step 206 , loss =  1.5181808471679688\n",
      "step 207 , loss =  1.5733966827392578\n",
      "step 208 , loss =  1.568587064743042\n",
      "step 209 , loss =  1.6446349620819092\n",
      "step 210 , loss =  1.5415927171707153\n",
      "step 211 , loss =  1.5400893688201904\n",
      "step 212 , loss =  1.4353288412094116\n",
      "step 213 , loss =  1.6677885055541992\n",
      "step 214 , loss =  1.5815843343734741\n",
      "step 215 , loss =  1.574111819267273\n",
      "step 216 , loss =  1.5356812477111816\n",
      "step 217 , loss =  1.6148531436920166\n",
      "step 218 , loss =  1.5588574409484863\n",
      "step 219 , loss =  1.5307118892669678\n",
      "step 220 , loss =  1.5820679664611816\n",
      "step 221 , loss =  1.6093432903289795\n",
      "step 222 , loss =  1.612276315689087\n",
      "step 223 , loss =  1.5930755138397217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 224 , loss =  1.6357378959655762\n",
      "step 225 , loss =  1.6076761484146118\n",
      "step 226 , loss =  1.614443302154541\n",
      "step 227 , loss =  1.4918248653411865\n",
      "step 228 , loss =  1.635043740272522\n",
      "step 229 , loss =  1.4426594972610474\n",
      "step 230 , loss =  1.502551794052124\n",
      "step 231 , loss =  1.5828381776809692\n",
      "step 232 , loss =  1.462816834449768\n",
      "step 233 , loss =  1.6099741458892822\n",
      "step 234 , loss =  1.4631117582321167\n",
      "step 235 , loss =  1.536323070526123\n",
      "step 236 , loss =  1.4684323072433472\n",
      "step 237 , loss =  1.5645387172698975\n",
      "step 238 , loss =  1.50203275680542\n",
      "step 239 , loss =  1.579676866531372\n",
      "step 240 , loss =  1.5729241371154785\n",
      "step 241 , loss =  1.6617279052734375\n",
      "step 242 , loss =  1.5832483768463135\n",
      "step 243 , loss =  1.5352771282196045\n",
      "step 244 , loss =  1.4300994873046875\n",
      "step 245 , loss =  1.6221139430999756\n",
      "step 246 , loss =  1.4040015935897827\n",
      "step 247 , loss =  1.6571340560913086\n",
      "step 248 , loss =  1.6719486713409424\n",
      "step 249 , loss =  1.643936038017273\n",
      "step 250 , loss =  1.6121394634246826\n",
      "step 251 , loss =  1.499939203262329\n",
      "step 252 , loss =  1.4918655157089233\n",
      "step 253 , loss =  1.5542397499084473\n",
      "step 254 , loss =  1.7079846858978271\n",
      "step 255 , loss =  1.4590657949447632\n",
      "step 256 , loss =  1.5596344470977783\n",
      "step 257 , loss =  1.4386265277862549\n",
      "step 258 , loss =  1.5372095108032227\n",
      "step 259 , loss =  1.561798334121704\n",
      "step 260 , loss =  1.4870796203613281\n",
      "step 261 , loss =  1.5086510181427002\n",
      "step 262 , loss =  1.586296558380127\n",
      "step 263 , loss =  1.4611458778381348\n",
      "step 264 , loss =  1.5821471214294434\n",
      "step 265 , loss =  1.5886574983596802\n",
      "step 266 , loss =  1.480426549911499\n",
      "step 267 , loss =  1.604567289352417\n",
      "step 268 , loss =  1.6153438091278076\n",
      "step 269 , loss =  1.5471727848052979\n",
      "step 270 , loss =  1.6817927360534668\n",
      "step 271 , loss =  1.594061017036438\n",
      "step 272 , loss =  1.4993836879730225\n",
      "step 273 , loss =  1.6007249355316162\n",
      "step 274 , loss =  1.4842376708984375\n",
      "step 275 , loss =  1.4738813638687134\n",
      "step 276 , loss =  1.4503462314605713\n",
      "step 277 , loss =  1.5506534576416016\n",
      "step 278 , loss =  1.4936543703079224\n",
      "step 279 , loss =  1.5600166320800781\n",
      "step 280 , loss =  1.4621846675872803\n",
      "step 281 , loss =  1.5606725215911865\n",
      "step 282 , loss =  1.5337791442871094\n",
      "step 283 , loss =  1.6211628913879395\n",
      "step 284 , loss =  1.6087040901184082\n",
      "step 285 , loss =  1.4403555393218994\n",
      "step 286 , loss =  1.4722731113433838\n",
      "step 287 , loss =  1.4522653818130493\n",
      "step 288 , loss =  1.4982755184173584\n",
      "step 289 , loss =  1.4643464088439941\n",
      "step 290 , loss =  1.4710395336151123\n",
      "step 291 , loss =  1.5441818237304688\n",
      "step 292 , loss =  1.4977179765701294\n",
      "step 293 , loss =  1.434062123298645\n",
      "step 294 , loss =  1.536745309829712\n",
      "step 295 , loss =  1.5424473285675049\n",
      "step 296 , loss =  1.4762532711029053\n",
      "step 297 , loss =  1.5549404621124268\n",
      "step 298 , loss =  1.3695459365844727\n",
      "step 299 , loss =  1.5663484334945679\n",
      "step 300 , loss =  1.519797444343567\n",
      "step 301 , loss =  1.5136806964874268\n",
      "step 302 , loss =  1.5356645584106445\n",
      "step 303 , loss =  1.4458751678466797\n",
      "step 304 , loss =  1.5219851732254028\n",
      "step 305 , loss =  1.4757423400878906\n",
      "step 306 , loss =  1.4484676122665405\n",
      "step 307 , loss =  1.440169334411621\n",
      "step 308 , loss =  1.5440175533294678\n",
      "step 309 , loss =  1.4899072647094727\n",
      "step 310 , loss =  1.4737975597381592\n",
      "step 311 , loss =  1.557490587234497\n",
      "step 312 , loss =  1.510474443435669\n",
      "step 313 , loss =  1.5026684999465942\n",
      "step 314 , loss =  1.4747899770736694\n",
      "step 315 , loss =  1.5031113624572754\n",
      "step 316 , loss =  1.4729300737380981\n",
      "step 317 , loss =  1.4096800088882446\n",
      "step 318 , loss =  1.5310907363891602\n",
      "step 319 , loss =  1.310636281967163\n",
      "step 320 , loss =  1.517760157585144\n",
      "step 321 , loss =  1.5035600662231445\n",
      "step 322 , loss =  1.4911377429962158\n",
      "step 323 , loss =  1.5111863613128662\n",
      "step 324 , loss =  1.4495197534561157\n",
      "step 325 , loss =  1.535322904586792\n",
      "step 326 , loss =  1.465783953666687\n",
      "step 327 , loss =  1.4184470176696777\n",
      "step 328 , loss =  1.3360689878463745\n",
      "step 329 , loss =  1.4421634674072266\n",
      "step 330 , loss =  1.552066683769226\n",
      "step 331 , loss =  1.4891541004180908\n",
      "step 332 , loss =  1.4272289276123047\n",
      "step 333 , loss =  1.4458024501800537\n",
      "step 334 , loss =  1.417321801185608\n",
      "step 335 , loss =  1.4871814250946045\n",
      "step 336 , loss =  1.4343323707580566\n",
      "step 337 , loss =  1.4264836311340332\n",
      "step 338 , loss =  1.3887701034545898\n",
      "step 339 , loss =  1.4264988899230957\n",
      "step 340 , loss =  1.5008435249328613\n",
      "step 341 , loss =  1.5574629306793213\n",
      "step 342 , loss =  1.4653319120407104\n",
      "step 343 , loss =  1.4165198802947998\n",
      "step 344 , loss =  1.4838194847106934\n",
      "step 345 , loss =  1.5003918409347534\n",
      "step 346 , loss =  1.5273501873016357\n",
      "step 347 , loss =  1.3782958984375\n",
      "step 348 , loss =  1.5236973762512207\n",
      "step 349 , loss =  1.5309492349624634\n",
      "step 350 , loss =  1.349106788635254\n",
      "step 351 , loss =  1.5542134046554565\n",
      "step 352 , loss =  1.4323827028274536\n",
      "step 353 , loss =  1.498455286026001\n",
      "step 354 , loss =  1.442719578742981\n",
      "step 355 , loss =  1.5854625701904297\n",
      "step 356 , loss =  1.432300329208374\n",
      "step 357 , loss =  1.5454387664794922\n",
      "step 358 , loss =  1.5140295028686523\n",
      "step 359 , loss =  1.4419623613357544\n",
      "step 360 , loss =  1.4754502773284912\n",
      "step 361 , loss =  1.5787746906280518\n",
      "step 362 , loss =  1.4780299663543701\n",
      "step 363 , loss =  1.4283270835876465\n",
      "step 364 , loss =  1.4959356784820557\n",
      "step 365 , loss =  1.4052917957305908\n",
      "step 366 , loss =  1.4449440240859985\n",
      "step 367 , loss =  1.5062870979309082\n",
      "step 368 , loss =  1.4195696115493774\n",
      "step 369 , loss =  1.5691009759902954\n",
      "step 370 , loss =  1.3989495038986206\n",
      "step 371 , loss =  1.5205994844436646\n",
      "step 372 , loss =  1.5801987648010254\n",
      "step 373 , loss =  1.3644851446151733\n",
      "step 374 , loss =  1.564380407333374\n",
      "step 375 , loss =  1.54719877243042\n",
      "step 376 , loss =  1.5452630519866943\n",
      "step 377 , loss =  1.494238257408142\n",
      "step 378 , loss =  1.377258062362671\n",
      "step 379 , loss =  1.448427438735962\n",
      "step 380 , loss =  1.4358241558074951\n",
      "step 381 , loss =  1.494551181793213\n",
      "step 382 , loss =  1.4339936971664429\n",
      "step 383 , loss =  1.5093779563903809\n",
      "step 384 , loss =  1.4903513193130493\n",
      "step 385 , loss =  1.4495978355407715\n",
      "step 386 , loss =  1.3377530574798584\n",
      "step 387 , loss =  1.460924506187439\n",
      "step 388 , loss =  1.3862061500549316\n",
      "step 389 , loss =  1.3426119089126587\n",
      "step 390 , loss =  1.4636201858520508\n",
      "step 391 , loss =  1.382047176361084\n",
      "step 392 , loss =  1.4273982048034668\n",
      "step 393 , loss =  1.4045982360839844\n",
      "step 394 , loss =  1.4618343114852905\n",
      "step 395 , loss =  1.3528215885162354\n",
      "step 396 , loss =  1.4626517295837402\n",
      "step 397 , loss =  1.4103450775146484\n",
      "step 398 , loss =  1.4159854650497437\n",
      "step 399 , loss =  1.3969841003417969\n",
      "step 400 , loss =  1.5054001808166504\n",
      "step 401 , loss =  1.545622706413269\n",
      "step 402 , loss =  1.5492900609970093\n",
      "step 403 , loss =  1.465443730354309\n",
      "step 404 , loss =  1.4223921298980713\n",
      "step 405 , loss =  1.4635255336761475\n",
      "step 406 , loss =  1.5652580261230469\n",
      "step 407 , loss =  1.4633653163909912\n",
      "step 408 , loss =  1.5711662769317627\n",
      "step 409 , loss =  1.463569164276123\n",
      "step 410 , loss =  1.4585323333740234\n",
      "step 411 , loss =  1.5153099298477173\n",
      "step 412 , loss =  1.5197890996932983\n",
      "step 413 , loss =  1.4104877710342407\n",
      "step 414 , loss =  1.5020625591278076\n",
      "step 415 , loss =  1.5367541313171387\n",
      "step 416 , loss =  1.4591561555862427\n",
      "step 417 , loss =  1.418662428855896\n",
      "step 418 , loss =  1.4637103080749512\n",
      "step 419 , loss =  1.5094319581985474\n",
      "step 420 , loss =  1.5529743432998657\n",
      "step 421 , loss =  1.4908578395843506\n",
      "step 422 , loss =  1.451296091079712\n",
      "step 423 , loss =  1.4665296077728271\n",
      "step 424 , loss =  1.466386318206787\n",
      "step 425 , loss =  1.4268884658813477\n",
      "step 426 , loss =  1.5442432165145874\n",
      "step 427 , loss =  1.468356728553772\n",
      "step 428 , loss =  1.3766252994537354\n",
      "step 429 , loss =  1.3036822080612183\n",
      "step 430 , loss =  1.3021029233932495\n",
      "step 431 , loss =  1.3665391206741333\n",
      "step 432 , loss =  1.3151404857635498\n",
      "step 433 , loss =  1.3404135704040527\n",
      "step 434 , loss =  1.365853190422058\n",
      "step 435 , loss =  1.3624346256256104\n",
      "step 436 , loss =  1.4537004232406616\n",
      "step 437 , loss =  1.4448542594909668\n",
      "step 438 , loss =  1.5692040920257568\n",
      "step 439 , loss =  1.3823426961898804\n",
      "step 440 , loss =  1.493887186050415\n",
      "step 441 , loss =  1.4036983251571655\n",
      "step 442 , loss =  1.5007288455963135\n",
      "step 443 , loss =  1.4489092826843262\n",
      "step 444 , loss =  1.4321057796478271\n",
      "step 445 , loss =  1.4496424198150635\n",
      "step 446 , loss =  1.4048330783843994\n",
      "step 447 , loss =  1.2976528406143188\n",
      "step 448 , loss =  1.5217547416687012\n",
      "step 449 , loss =  1.4250048398971558\n",
      "step 450 , loss =  1.3608471155166626\n",
      "step 451 , loss =  1.3888438940048218\n",
      "step 452 , loss =  1.363614797592163\n",
      "step 453 , loss =  1.439645528793335\n",
      "step 454 , loss =  1.3740177154541016\n",
      "step 455 , loss =  1.4611248970031738\n",
      "step 456 , loss =  1.5567023754119873\n",
      "step 457 , loss =  1.3529314994812012\n",
      "step 458 , loss =  1.6069823503494263\n",
      "step 459 , loss =  1.5290074348449707\n",
      "step 460 , loss =  1.3755848407745361\n",
      "step 461 , loss =  1.459765911102295\n",
      "step 462 , loss =  1.3738960027694702\n",
      "step 463 , loss =  1.5224900245666504\n",
      "step 464 , loss =  1.4813411235809326\n",
      "step 465 , loss =  1.4312410354614258\n",
      "step 466 , loss =  1.435430884361267\n",
      "step 467 , loss =  1.5700149536132812\n",
      "step 468 , loss =  1.4403576850891113\n",
      "step 469 , loss =  1.518935203552246\n",
      "step 470 , loss =  1.2725275754928589\n",
      "step 471 , loss =  1.4814815521240234\n",
      "step 472 , loss =  1.4663569927215576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 473 , loss =  1.5096774101257324\n",
      "step 474 , loss =  1.4917936325073242\n",
      "step 475 , loss =  1.4625539779663086\n",
      "step 476 , loss =  1.452277660369873\n",
      "step 477 , loss =  1.5250884294509888\n",
      "step 478 , loss =  1.422173261642456\n",
      "step 479 , loss =  1.3751426935195923\n",
      "step 480 , loss =  1.5598092079162598\n",
      "step 481 , loss =  1.5745118856430054\n",
      "step 482 , loss =  1.4099211692810059\n",
      "step 483 , loss =  1.514366626739502\n",
      "step 484 , loss =  1.6100928783416748\n",
      "step 485 , loss =  1.4522786140441895\n",
      "step 486 , loss =  1.4250667095184326\n",
      "step 487 , loss =  1.5019526481628418\n",
      "step 488 , loss =  1.520362377166748\n",
      "step 489 , loss =  1.343189001083374\n",
      "step 490 , loss =  1.5157513618469238\n",
      "step 491 , loss =  1.4695336818695068\n",
      "step 492 , loss =  1.4259767532348633\n",
      "step 493 , loss =  1.3587489128112793\n",
      "step 494 , loss =  1.4761300086975098\n",
      "step 495 , loss =  1.5037891864776611\n",
      "step 496 , loss =  1.4938222169876099\n",
      "step 497 , loss =  1.497666597366333\n",
      "step 498 , loss =  1.5001895427703857\n",
      "step 499 , loss =  1.3705332279205322\n",
      "step 500 , loss =  1.4983837604522705\n",
      "step 501 , loss =  1.5995314121246338\n",
      "step 502 , loss =  1.4924354553222656\n",
      "step 503 , loss =  1.339864730834961\n",
      "step 504 , loss =  1.5469551086425781\n",
      "step 505 , loss =  1.408920168876648\n",
      "step 506 , loss =  1.3964327573776245\n",
      "step 507 , loss =  1.4209465980529785\n",
      "step 508 , loss =  1.5178505182266235\n",
      "step 509 , loss =  1.3860937356948853\n",
      "step 510 , loss =  1.4017250537872314\n",
      "step 511 , loss =  1.5757251977920532\n",
      "step 512 , loss =  1.4442625045776367\n",
      "step 513 , loss =  1.340282917022705\n",
      "step 514 , loss =  1.4197131395339966\n",
      "step 515 , loss =  1.4929369688034058\n",
      "step 516 , loss =  1.4308254718780518\n",
      "step 517 , loss =  1.415750503540039\n",
      "step 518 , loss =  1.3868107795715332\n",
      "step 519 , loss =  1.4465218782424927\n",
      "step 520 , loss =  1.4779770374298096\n",
      "step 521 , loss =  1.2983880043029785\n",
      "step 522 , loss =  1.503206491470337\n",
      "step 523 , loss =  1.359811544418335\n",
      "step 524 , loss =  1.4423494338989258\n",
      "step 525 , loss =  1.417171835899353\n",
      "step 526 , loss =  1.419840931892395\n",
      "step 527 , loss =  1.3535993099212646\n",
      "step 528 , loss =  1.4581717252731323\n",
      "step 529 , loss =  1.4516960382461548\n",
      "step 530 , loss =  1.5169183015823364\n",
      "step 531 , loss =  1.443465232849121\n",
      "step 532 , loss =  1.3870573043823242\n",
      "step 533 , loss =  1.4018527269363403\n",
      "step 534 , loss =  1.4294579029083252\n",
      "step 535 , loss =  1.5026955604553223\n",
      "step 536 , loss =  1.436348557472229\n",
      "step 537 , loss =  1.404187560081482\n",
      "step 538 , loss =  1.2851051092147827\n",
      "step 539 , loss =  1.5178823471069336\n",
      "step 540 , loss =  1.3134368658065796\n",
      "step 541 , loss =  1.5011337995529175\n",
      "step 542 , loss =  1.332718849182129\n",
      "step 543 , loss =  1.3955554962158203\n",
      "step 544 , loss =  1.3567230701446533\n",
      "step 545 , loss =  1.3767199516296387\n",
      "step 546 , loss =  1.4601150751113892\n",
      "step 547 , loss =  1.3680872917175293\n",
      "step 548 , loss =  1.4111405611038208\n",
      "step 549 , loss =  1.2474358081817627\n",
      "step 550 , loss =  1.4097483158111572\n",
      "step 551 , loss =  1.452585220336914\n",
      "step 552 , loss =  1.3471169471740723\n",
      "step 553 , loss =  1.3566402196884155\n",
      "step 554 , loss =  1.4945249557495117\n",
      "step 555 , loss =  1.4226620197296143\n",
      "step 556 , loss =  1.3195250034332275\n",
      "step 557 , loss =  1.4567208290100098\n",
      "step 558 , loss =  1.3684030771255493\n",
      "step 559 , loss =  1.3894745111465454\n",
      "step 560 , loss =  1.418028473854065\n",
      "step 561 , loss =  1.3211239576339722\n",
      "step 562 , loss =  1.2586629390716553\n",
      "step 563 , loss =  1.499114990234375\n",
      "step 564 , loss =  1.394503116607666\n",
      "step 565 , loss =  1.3731342554092407\n",
      "step 566 , loss =  1.4225910902023315\n",
      "step 567 , loss =  1.3372972011566162\n",
      "step 568 , loss =  1.2214479446411133\n",
      "step 569 , loss =  1.3164424896240234\n",
      "step 570 , loss =  1.4174399375915527\n",
      "step 571 , loss =  1.4385144710540771\n",
      "step 572 , loss =  1.4922794103622437\n",
      "step 573 , loss =  1.3237378597259521\n",
      "step 574 , loss =  1.3541475534439087\n",
      "step 575 , loss =  1.4052733182907104\n",
      "step 576 , loss =  1.4587820768356323\n",
      "step 577 , loss =  1.3586971759796143\n",
      "step 578 , loss =  1.481092929840088\n",
      "step 579 , loss =  1.412485122680664\n",
      "step 580 , loss =  1.4579194784164429\n",
      "step 581 , loss =  1.3188624382019043\n",
      "step 582 , loss =  1.3996392488479614\n",
      "step 583 , loss =  1.5072927474975586\n",
      "step 584 , loss =  1.3836829662322998\n",
      "step 585 , loss =  1.582010269165039\n",
      "step 586 , loss =  1.3856635093688965\n",
      "step 587 , loss =  1.4050002098083496\n",
      "step 588 , loss =  1.2694714069366455\n",
      "step 589 , loss =  1.483756422996521\n",
      "step 590 , loss =  1.322415828704834\n",
      "step 591 , loss =  1.350191354751587\n",
      "step 592 , loss =  1.3340442180633545\n",
      "step 593 , loss =  1.3232892751693726\n",
      "step 594 , loss =  1.3247158527374268\n",
      "step 595 , loss =  1.372730016708374\n",
      "step 596 , loss =  1.319211721420288\n",
      "step 597 , loss =  1.3609342575073242\n",
      "step 598 , loss =  1.2932202816009521\n",
      "step 599 , loss =  1.4254388809204102\n",
      "step 600 , loss =  1.378143548965454\n",
      "step 601 , loss =  1.2685538530349731\n",
      "step 602 , loss =  1.2846564054489136\n",
      "step 603 , loss =  1.361039161682129\n",
      "step 604 , loss =  1.2543946504592896\n",
      "step 605 , loss =  1.3848896026611328\n",
      "step 606 , loss =  1.4096770286560059\n",
      "step 607 , loss =  1.3783116340637207\n",
      "step 608 , loss =  1.4131391048431396\n",
      "step 609 , loss =  1.454330325126648\n",
      "step 610 , loss =  1.409830093383789\n",
      "step 611 , loss =  1.4027493000030518\n",
      "step 612 , loss =  1.3289300203323364\n",
      "step 613 , loss =  1.3425885438919067\n",
      "step 614 , loss =  1.3715354204177856\n",
      "step 615 , loss =  1.4475992918014526\n",
      "step 616 , loss =  1.2567152976989746\n",
      "step 617 , loss =  1.536133050918579\n",
      "step 618 , loss =  1.309821605682373\n",
      "step 619 , loss =  1.4295029640197754\n",
      "step 620 , loss =  1.3219045400619507\n",
      "step 621 , loss =  1.3647032976150513\n",
      "step 622 , loss =  1.3866662979125977\n",
      "step 623 , loss =  1.494698405265808\n",
      "step 624 , loss =  1.4029605388641357\n",
      "step 625 , loss =  1.2862284183502197\n",
      "step 626 , loss =  1.4253571033477783\n",
      "step 627 , loss =  1.3737258911132812\n",
      "step 628 , loss =  1.151411533355713\n",
      "step 629 , loss =  1.3713057041168213\n",
      "step 630 , loss =  1.5473144054412842\n",
      "step 631 , loss =  1.375807762145996\n",
      "step 632 , loss =  1.378948450088501\n",
      "step 633 , loss =  1.3701627254486084\n",
      "step 634 , loss =  1.4006707668304443\n",
      "step 635 , loss =  1.4689137935638428\n",
      "step 636 , loss =  1.4320038557052612\n",
      "step 637 , loss =  1.3212077617645264\n",
      "step 638 , loss =  1.368238925933838\n",
      "step 639 , loss =  1.3809361457824707\n",
      "step 640 , loss =  1.44768488407135\n",
      "step 641 , loss =  1.5799288749694824\n",
      "step 642 , loss =  1.3925707340240479\n",
      "step 643 , loss =  1.3498706817626953\n",
      "step 644 , loss =  1.3409193754196167\n",
      "step 645 , loss =  1.4835050106048584\n",
      "step 646 , loss =  1.4198070764541626\n",
      "step 647 , loss =  1.436859369277954\n",
      "step 648 , loss =  1.3322293758392334\n",
      "step 649 , loss =  1.4156098365783691\n",
      "step 650 , loss =  1.4133509397506714\n",
      "step 651 , loss =  1.3369404077529907\n",
      "step 652 , loss =  1.401782751083374\n",
      "step 653 , loss =  1.3470609188079834\n",
      "step 654 , loss =  1.3311140537261963\n",
      "step 655 , loss =  1.3649672269821167\n",
      "step 656 , loss =  1.4291203022003174\n",
      "step 657 , loss =  1.3250341415405273\n",
      "step 658 , loss =  1.5121245384216309\n",
      "step 659 , loss =  1.5376036167144775\n",
      "step 660 , loss =  1.4446849822998047\n",
      "step 661 , loss =  1.3856868743896484\n",
      "step 662 , loss =  1.3506908416748047\n",
      "step 663 , loss =  1.4169921875\n",
      "step 664 , loss =  1.3444453477859497\n",
      "step 665 , loss =  1.3519600629806519\n",
      "step 666 , loss =  1.307468056678772\n",
      "step 667 , loss =  1.235834002494812\n",
      "step 668 , loss =  1.3020161390304565\n",
      "step 669 , loss =  1.4748997688293457\n",
      "step 670 , loss =  1.458342432975769\n",
      "step 671 , loss =  1.2821629047393799\n",
      "step 672 , loss =  1.3952815532684326\n",
      "step 673 , loss =  1.5119667053222656\n",
      "step 674 , loss =  1.3370614051818848\n",
      "step 675 , loss =  1.3513731956481934\n",
      "step 676 , loss =  1.4342730045318604\n",
      "step 677 , loss =  1.3211930990219116\n",
      "step 678 , loss =  1.417616367340088\n",
      "step 679 , loss =  1.4795074462890625\n",
      "step 680 , loss =  1.3393964767456055\n",
      "step 681 , loss =  1.3076202869415283\n",
      "step 682 , loss =  1.4205470085144043\n",
      "step 683 , loss =  1.329106092453003\n",
      "step 684 , loss =  1.344095230102539\n",
      "step 685 , loss =  1.3687982559204102\n",
      "step 686 , loss =  1.4898362159729004\n",
      "step 687 , loss =  1.270374059677124\n",
      "step 688 , loss =  1.3054895401000977\n",
      "step 689 , loss =  1.2908720970153809\n",
      "step 690 , loss =  1.313276767730713\n",
      "step 691 , loss =  1.385282278060913\n",
      "step 692 , loss =  1.3907358646392822\n",
      "step 693 , loss =  1.2966516017913818\n",
      "step 694 , loss =  1.2947256565093994\n",
      "step 695 , loss =  1.2677618265151978\n",
      "step 696 , loss =  1.3854619264602661\n",
      "step 697 , loss =  1.3713750839233398\n",
      "step 698 , loss =  1.4633636474609375\n",
      "step 699 , loss =  1.3033149242401123\n",
      "step 700 , loss =  1.2970070838928223\n",
      "step 701 , loss =  1.3439151048660278\n",
      "step 702 , loss =  1.3478422164916992\n",
      "step 703 , loss =  1.4181809425354004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 704 , loss =  1.351839542388916\n",
      "step 705 , loss =  1.357790470123291\n",
      "step 706 , loss =  1.3266236782073975\n",
      "step 707 , loss =  1.2991738319396973\n",
      "step 708 , loss =  1.421311855316162\n",
      "step 709 , loss =  1.3545928001403809\n",
      "step 710 , loss =  1.413337230682373\n",
      "step 711 , loss =  1.4180176258087158\n",
      "step 712 , loss =  1.465873122215271\n",
      "step 713 , loss =  1.3398754596710205\n",
      "step 714 , loss =  1.31160306930542\n",
      "step 715 , loss =  1.3461366891860962\n",
      "step 716 , loss =  1.3446271419525146\n",
      "step 717 , loss =  1.2679541110992432\n",
      "step 718 , loss =  1.446315050125122\n",
      "step 719 , loss =  1.3402596712112427\n",
      "step 720 , loss =  1.2868695259094238\n",
      "step 721 , loss =  1.3785521984100342\n",
      "step 722 , loss =  1.3497563600540161\n",
      "step 723 , loss =  1.2749032974243164\n",
      "step 724 , loss =  1.3729417324066162\n",
      "step 725 , loss =  1.3200230598449707\n",
      "step 726 , loss =  1.449237585067749\n",
      "step 727 , loss =  1.4013032913208008\n",
      "step 728 , loss =  1.3648228645324707\n",
      "step 729 , loss =  1.4189505577087402\n",
      "step 730 , loss =  1.320781946182251\n",
      "step 731 , loss =  1.4153273105621338\n",
      "step 732 , loss =  1.5018699169158936\n",
      "step 733 , loss =  1.2209744453430176\n",
      "step 734 , loss =  1.5100088119506836\n",
      "step 735 , loss =  1.299551010131836\n",
      "step 736 , loss =  1.3587113618850708\n",
      "step 737 , loss =  1.2588376998901367\n",
      "step 738 , loss =  1.2254658937454224\n",
      "step 739 , loss =  1.2893089056015015\n",
      "step 740 , loss =  1.216507911682129\n",
      "step 741 , loss =  1.220231056213379\n",
      "step 742 , loss =  1.3071444034576416\n",
      "step 743 , loss =  1.2078053951263428\n",
      "step 744 , loss =  1.230323314666748\n",
      "step 745 , loss =  1.3568966388702393\n",
      "step 746 , loss =  1.2600046396255493\n",
      "step 747 , loss =  1.2950173616409302\n",
      "step 748 , loss =  1.3655431270599365\n",
      "step 749 , loss =  1.3181788921356201\n",
      "step 750 , loss =  1.2846009731292725\n",
      "step 751 , loss =  1.3318424224853516\n",
      "step 752 , loss =  1.333137035369873\n",
      "step 753 , loss =  1.3321518898010254\n",
      "step 754 , loss =  1.2571086883544922\n",
      "step 755 , loss =  1.4889849424362183\n",
      "step 756 , loss =  1.3298094272613525\n",
      "step 757 , loss =  1.3033994436264038\n",
      "step 758 , loss =  1.3563859462738037\n",
      "step 759 , loss =  1.3522298336029053\n",
      "step 760 , loss =  1.3531467914581299\n",
      "step 761 , loss =  1.3378968238830566\n",
      "step 762 , loss =  1.4637341499328613\n",
      "step 763 , loss =  1.4114265441894531\n",
      "step 764 , loss =  1.317223072052002\n",
      "step 765 , loss =  1.4402364492416382\n",
      "step 766 , loss =  1.4364511966705322\n",
      "step 767 , loss =  1.3031294345855713\n",
      "step 768 , loss =  1.3494415283203125\n",
      "step 769 , loss =  1.3305583000183105\n",
      "step 770 , loss =  1.2869226932525635\n",
      "step 771 , loss =  1.4905345439910889\n",
      "step 772 , loss =  1.523490309715271\n",
      "step 773 , loss =  1.324927568435669\n",
      "step 774 , loss =  1.3382127285003662\n",
      "step 775 , loss =  1.3959681987762451\n",
      "step 776 , loss =  1.2926081418991089\n",
      "step 777 , loss =  1.3941386938095093\n",
      "step 778 , loss =  1.3344039916992188\n",
      "step 779 , loss =  1.3131985664367676\n",
      "step 780 , loss =  1.2806644439697266\n",
      "step 781 , loss =  1.3289153575897217\n",
      "step 782 , loss =  1.2898167371749878\n",
      "step 783 , loss =  1.4036228656768799\n",
      "step 784 , loss =  1.2670793533325195\n",
      "step 785 , loss =  1.322582721710205\n",
      "step 786 , loss =  1.3161864280700684\n",
      "step 787 , loss =  1.293492078781128\n",
      "step 788 , loss =  1.425514578819275\n",
      "step 789 , loss =  1.4053884744644165\n",
      "step 790 , loss =  1.211022973060608\n",
      "step 791 , loss =  1.265453815460205\n",
      "step 792 , loss =  1.3111066818237305\n",
      "step 793 , loss =  1.2915217876434326\n",
      "step 794 , loss =  1.2764586210250854\n",
      "step 795 , loss =  1.2848639488220215\n",
      "step 796 , loss =  1.3351495265960693\n",
      "step 797 , loss =  1.339362621307373\n",
      "step 798 , loss =  1.2710866928100586\n",
      "step 799 , loss =  1.246675729751587\n",
      "step 800 , loss =  1.332857608795166\n",
      "step 801 , loss =  1.3975845575332642\n",
      "step 802 , loss =  1.3838801383972168\n",
      "step 803 , loss =  1.249690055847168\n",
      "step 804 , loss =  1.367236614227295\n",
      "step 805 , loss =  1.302984356880188\n",
      "step 806 , loss =  1.2724740505218506\n",
      "step 807 , loss =  1.2943921089172363\n",
      "step 808 , loss =  1.4348732233047485\n",
      "step 809 , loss =  1.1915283203125\n",
      "step 810 , loss =  1.334188461303711\n",
      "step 811 , loss =  1.2966148853302002\n",
      "step 812 , loss =  1.3125510215759277\n",
      "step 813 , loss =  1.30026376247406\n",
      "step 814 , loss =  1.3121347427368164\n",
      "step 815 , loss =  1.148449420928955\n",
      "step 816 , loss =  1.346592664718628\n",
      "step 817 , loss =  1.3207868337631226\n",
      "step 818 , loss =  1.3195512294769287\n",
      "step 819 , loss =  1.3648159503936768\n",
      "step 820 , loss =  1.3219685554504395\n",
      "step 821 , loss =  1.2671319246292114\n",
      "step 822 , loss =  1.39267098903656\n",
      "step 823 , loss =  1.3789780139923096\n",
      "step 824 , loss =  1.413328766822815\n",
      "step 825 , loss =  1.2783012390136719\n",
      "step 826 , loss =  1.3466873168945312\n",
      "step 827 , loss =  1.2566872835159302\n",
      "step 828 , loss =  1.3412578105926514\n",
      "step 829 , loss =  1.3347070217132568\n",
      "step 830 , loss =  1.2173640727996826\n",
      "step 831 , loss =  1.2792630195617676\n",
      "step 832 , loss =  1.2819688320159912\n",
      "step 833 , loss =  1.3516945838928223\n",
      "step 834 , loss =  1.314725637435913\n",
      "step 835 , loss =  1.308142066001892\n",
      "step 836 , loss =  1.1929851770401\n",
      "step 837 , loss =  1.3037068843841553\n",
      "step 838 , loss =  1.3671221733093262\n",
      "step 839 , loss =  1.2028377056121826\n",
      "step 840 , loss =  1.26608145236969\n",
      "step 841 , loss =  1.2097328901290894\n",
      "step 842 , loss =  1.2727851867675781\n",
      "step 843 , loss =  1.2605221271514893\n",
      "step 844 , loss =  1.2774510383605957\n",
      "step 845 , loss =  1.2740753889083862\n",
      "step 846 , loss =  1.2627272605895996\n",
      "step 847 , loss =  1.2765655517578125\n",
      "step 848 , loss =  1.3032734394073486\n",
      "step 849 , loss =  1.184237003326416\n",
      "step 850 , loss =  1.2186863422393799\n",
      "step 851 , loss =  1.2073543071746826\n",
      "step 852 , loss =  1.278519630432129\n",
      "step 853 , loss =  1.3836400508880615\n",
      "step 854 , loss =  1.1888389587402344\n",
      "step 855 , loss =  1.2982120513916016\n",
      "step 856 , loss =  1.2602461576461792\n",
      "step 857 , loss =  1.2522122859954834\n",
      "step 858 , loss =  1.2226603031158447\n",
      "step 859 , loss =  1.388730764389038\n",
      "step 860 , loss =  1.2591139078140259\n",
      "step 861 , loss =  1.168753981590271\n",
      "step 862 , loss =  1.3148281574249268\n",
      "step 863 , loss =  1.2857129573822021\n",
      "step 864 , loss =  1.2464853525161743\n",
      "step 865 , loss =  1.2537109851837158\n",
      "step 866 , loss =  1.2613670825958252\n",
      "step 867 , loss =  1.2845432758331299\n",
      "step 868 , loss =  1.2507081031799316\n",
      "step 869 , loss =  1.2882152795791626\n",
      "step 870 , loss =  1.2898677587509155\n",
      "step 871 , loss =  1.2589417695999146\n",
      "step 872 , loss =  1.204498052597046\n",
      "step 873 , loss =  1.2030993700027466\n",
      "step 874 , loss =  1.2692418098449707\n",
      "step 875 , loss =  1.3199859857559204\n",
      "step 876 , loss =  1.2425787448883057\n",
      "step 877 , loss =  1.2197189331054688\n",
      "step 878 , loss =  1.2716554403305054\n",
      "step 879 , loss =  1.2725355625152588\n",
      "step 880 , loss =  1.3586866855621338\n",
      "step 881 , loss =  1.3584986925125122\n",
      "step 882 , loss =  1.3106493949890137\n",
      "step 883 , loss =  1.3179510831832886\n",
      "step 884 , loss =  1.2734978199005127\n",
      "step 885 , loss =  1.204704761505127\n",
      "step 886 , loss =  1.241150140762329\n",
      "step 887 , loss =  1.253464698791504\n",
      "step 888 , loss =  1.3085908889770508\n",
      "step 889 , loss =  1.2357417345046997\n",
      "step 890 , loss =  1.275303840637207\n",
      "step 891 , loss =  1.2456878423690796\n",
      "step 892 , loss =  1.205153226852417\n",
      "step 893 , loss =  1.2447099685668945\n",
      "step 894 , loss =  1.2559926509857178\n",
      "step 895 , loss =  1.2761858701705933\n",
      "step 896 , loss =  1.3285679817199707\n",
      "step 897 , loss =  1.230513334274292\n",
      "step 898 , loss =  1.227684497833252\n",
      "step 899 , loss =  1.2156835794448853\n",
      "step 900 , loss =  1.2199831008911133\n",
      "step 901 , loss =  1.157597303390503\n",
      "step 902 , loss =  1.2004263401031494\n",
      "step 903 , loss =  1.2836976051330566\n",
      "step 904 , loss =  1.2841877937316895\n",
      "step 905 , loss =  1.3410981893539429\n",
      "step 906 , loss =  1.3030149936676025\n",
      "step 907 , loss =  1.1705906391143799\n",
      "step 908 , loss =  1.2552785873413086\n",
      "step 909 , loss =  1.1569126844406128\n",
      "step 910 , loss =  1.2075598239898682\n",
      "step 911 , loss =  1.091174840927124\n",
      "step 912 , loss =  1.1075527667999268\n",
      "step 913 , loss =  1.1956353187561035\n",
      "step 914 , loss =  1.125970482826233\n",
      "step 915 , loss =  1.1751675605773926\n",
      "step 916 , loss =  1.113645076751709\n",
      "step 917 , loss =  1.1032943725585938\n",
      "step 918 , loss =  1.2203024625778198\n",
      "step 919 , loss =  1.2582310438156128\n",
      "step 920 , loss =  1.2152726650238037\n",
      "step 921 , loss =  1.1425687074661255\n",
      "step 922 , loss =  1.1994531154632568\n",
      "step 923 , loss =  1.207477331161499\n",
      "step 924 , loss =  1.296126127243042\n",
      "step 925 , loss =  1.2490898370742798\n",
      "step 926 , loss =  1.2084205150604248\n",
      "step 927 , loss =  1.1488429307937622\n",
      "step 928 , loss =  1.2104389667510986\n",
      "step 929 , loss =  1.193495273590088\n",
      "step 930 , loss =  1.1962664127349854\n",
      "step 931 , loss =  1.189182996749878\n",
      "step 932 , loss =  1.2303481101989746\n",
      "step 933 , loss =  1.1641557216644287\n",
      "step 934 , loss =  1.1769545078277588\n",
      "step 935 , loss =  1.1896567344665527\n",
      "step 936 , loss =  1.1649961471557617\n",
      "step 937 , loss =  1.1771955490112305\n"
     ]
    }
   ],
   "source": [
    "for step, (x, y) in enumerate(dataset):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = linear_layer(x)\n",
    "        loss = loss_fn(y, logits)\n",
    "    gradients = tape.gradient(loss, linear_layer.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(gradients, linear_layer.trainable_weights))\n",
    "    if step % 1 == 0:\n",
    "        print(\"step\", step, \", loss = \", float(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60032"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "938*64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "?dataset.shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.data.Dataset.range(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ds.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.shuffle(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ShuffleDataset shapes: (), types: tf.int64>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1, 0]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ds.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "?ds.batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Dense(32, activation = tf.nn.relu),\n",
    "        keras.layers.Dense(32, activation = tf.nn.relu),\n",
    "        keras.layers.Dense(10)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivityRegularization(keras.layers.Layer):\n",
    "    def __init__(self, rate=1e-2):\n",
    "        super(ActivityRegularization, self).__init__()\n",
    "        self.rate = rate\n",
    "    def call(self, inputs):\n",
    "        self.add_loss(self.rate * tf.reduce_sum(inputs))\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseMLP(keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(SparseMLP, self).__init__()\n",
    "        self.linear_1 = Linear(32)\n",
    "        self.regularization = ActivityRegularization(1e-2)\n",
    "        self.linear_3 = Linear(10)\n",
    "        \n",
    "    def call(self, inputs1):\n",
    "        x = self.linear_1(inputs1)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.regularization(x)\n",
    "        return self.linear_3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = SparseMLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = mlp(tf.ones((10, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=0.2529298>]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 , loss =  2.3008975982666016\n",
      "step 100 , loss =  2.3098599910736084\n",
      "step 200 , loss =  2.298550844192505\n",
      "step 300 , loss =  2.297795534133911\n",
      "step 400 , loss =  2.304739236831665\n",
      "step 500 , loss =  2.309346914291382\n",
      "step 600 , loss =  2.3182613849639893\n",
      "step 700 , loss =  2.3104445934295654\n",
      "step 800 , loss =  2.3015530109405518\n",
      "step 900 , loss =  2.304327964782715\n"
     ]
    }
   ],
   "source": [
    "for step, (x, y) in enumerate(dataset):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = mlp(x)\n",
    "        loss = loss_fn(y, logits)\n",
    "        loss += sum(mlp.losses)\n",
    "    gradients = tape.gradient(loss, mlp.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(gradients, mlp.trainable_weights))\n",
    "    if step % 100 == 0:\n",
    "        print(\"step\", step, \", loss = \", float(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with statement: https://www.geeksforgeeks.org/with-statement-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 Loss: 2.38885498046875\n",
      "Step: 100 Loss: 0.6748959422111511\n",
      "Step: 200 Loss: 0.63692307472229\n",
      "Step: 300 Loss: 0.4861977994441986\n",
      "Step: 400 Loss: 0.13841241598129272\n",
      "Step: 500 Loss: 0.2590877413749695\n",
      "Step: 600 Loss: 0.29595351219177246\n",
      "Step: 700 Loss: 0.27203017473220825\n",
      "Step: 800 Loss: 0.19011381268501282\n",
      "Step: 900 Loss: 0.23012065887451172\n"
     ]
    }
   ],
   "source": [
    "# Prepare our layer, loss, and optimizer.\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Dense(32, activation=\"relu\"),\n",
    "        keras.layers.Dense(32, activation=\"relu\"),\n",
    "        keras.layers.Dense(10),\n",
    "    ]\n",
    ")\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "@tf.function\n",
    "def train_on_batch(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(x)\n",
    "        loss = loss_fn(y, logits)\n",
    "    gradients = tape.gradient(loss, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
    "    return loss\n",
    "\n",
    "(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train.reshape(60000, 784).astype(\"float32\") / 255, y_train)\n",
    ")\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "for step, (x, y) in enumerate(dataset):\n",
    "    loss = train_on_batch(x, y)\n",
    "    if step % 100 == 0:\n",
    "        print(\"Step:\", step, \"Loss:\", float(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout(keras.layers.Layer):\n",
    "    def __init__(self, rate):\n",
    "        super(Dropout, self).__init__()\n",
    "        self.rate = rate\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        if training:\n",
    "            return tf.nn.dropout(inputs, rate=self.rate)\n",
    "        return inputs\n",
    "\n",
    "\n",
    "class MLPWithDropout(keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(MLPWithDropout, self).__init__()\n",
    "        self.linear_1 = Linear(32)\n",
    "        self.dropout = Dropout(0.5)\n",
    "        self.linear_3 = Linear(10)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        x = self.linear_1(inputs)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.dropout(x, training=training)\n",
    "        return self.linear_3(x)\n",
    "\n",
    "\n",
    "mlp = MLPWithDropout()\n",
    "y_train = mlp(tf.ones((2, 2)), training=True)\n",
    "y_test = mlp(tf.ones((2, 2)), training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(16,), dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'input_1:0' shape=(None, 16) dtype=float32>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Linear(32)(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'linear_14/add:0' shape=(None, 32) dtype=float32>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Dropout(0.5)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dropout_1/dropout_1/Identity:0' shape=(None, 32) dtype=float32>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = Linear(10)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'linear_15/add:0' shape=(None, 10) dtype=float32>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 10), dtype=float32, numpy=\n",
       "array([[ 0.02364092,  0.04820701,  0.07519927, -0.04784147, -0.03294853,\n",
       "        -0.00909306, -0.12849325,  0.01039225,  0.11472937, -0.02940241],\n",
       "       [ 0.02364092,  0.04820701,  0.07519927, -0.04784147, -0.03294853,\n",
       "        -0.00909306, -0.12849325,  0.01039225,  0.11472937, -0.02940241]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(tf.ones((2, 16)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs # what's the shape of z_log_var, a scalar?\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(layers.Layer):\n",
    "    def __init__(self, latent_dim = 32, intermediate_dim = 64, **kwargs):\n",
    "        super(Encoder, self).__init__(**kwargs)\n",
    "        self.dense_proj = layers.Dense(intermediate_dim, activation = tf.nn.relu)\n",
    "        self.dense_mean = layers.Dense(latent_dim)\n",
    "        self.dense_log_var = layers.Dense(latent_dim)\n",
    "        self.sampling = Sampling()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.dense_proj(inputs)\n",
    "        z_mean = self.dense_mean(x)\n",
    "        z_log_var = self.dense_log_var(x) # so z_log_var has the same shape with z_mean\n",
    "        z = self.sampling((z_mean, z_log_var))\n",
    "        return z_mean, z_log_var, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(layers.Layer):\n",
    "    def __init__(self, original_dim, intermediate_dim=64, **kwargs):\n",
    "        super(Decoder, self).__init__(**kwargs)\n",
    "        self.dense_proj = layers.Dense(intermediate_dim, activation = tf.nn.relu)\n",
    "        self.dense_output = layers.Dense(original_dim, activation = tf.nn.sigmoid)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.dense_proj(inputs)\n",
    "        return self.dense_output(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoEncoder(layers.Layer):\n",
    "    def __init__(self, original_dim, intermediate_dim = 64, latent_dim = 32, **kwargs):\n",
    "        super(VariationalAutoEncoder, self).__init__(**kwargs)\n",
    "        self.original_dim = original_dim # not infer from the shape of inputs\n",
    "        self.encoder = Encoder(latent_dim=latent_dim, intermediate_dim = intermediate_dim)\n",
    "        self.decoder = Decoder(original_dim, intermediate_dim = intermediate_dim)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var, z = self.encoder(inputs)\n",
    "        reconstructed = self.decoder(z)\n",
    "        kl_loss = -0.5 * tf.reduce_mean(\n",
    "            z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1\n",
    "        )\n",
    "        self.add_loss(kl_loss)\n",
    "        return reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0 loss: 0.34750431776046753\n",
      "step: 100 loss: 0.12663604739573922\n",
      "step: 200 loss: 0.10030697182339815\n",
      "step: 300 loss: 0.08997227075785101\n",
      "step: 400 loss: 0.08488470015866204\n",
      "step: 500 loss: 0.08169266278158405\n",
      "step: 600 loss: 0.0792588794452081\n",
      "step: 700 loss: 0.0778793450365223\n",
      "step: 800 loss: 0.07667038871554399\n",
      "step: 900 loss: 0.0757216972843656\n",
      "step: 1000 loss: 0.0747777977427998\n",
      "step: 1100 loss: 0.0740617620813695\n",
      "step: 1200 loss: 0.07360787985909988\n",
      "step: 1300 loss: 0.07316073791040172\n",
      "step: 1400 loss: 0.07272018230308132\n",
      "step: 1500 loss: 0.07238143778528316\n",
      "step: 1600 loss: 0.07208472068452373\n",
      "step: 1700 loss: 0.07175387290778895\n",
      "step: 1800 loss: 0.07149329694863957\n"
     ]
    }
   ],
   "source": [
    "vae = VariationalAutoEncoder(original_dim=784, intermediate_dim=64, latent_dim=32)\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "(x_train, _), _ = tf.keras.datasets.mnist.load_data()\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    x_train.reshape(60000, 784).astype(\"float32\") / 255\n",
    ")\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(32)\n",
    "\n",
    "@tf.function\n",
    "def training_step(x):\n",
    "    with tf.GradientTape() as tape:\n",
    "        reconstructed = vae(x)\n",
    "        loss = loss_fn(x, reconstructed)\n",
    "        loss += sum(vae.losses)\n",
    "    grads = tape.gradient(loss, vae.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, vae.trainable_weights))\n",
    "    return loss\n",
    "\n",
    "losses = []\n",
    "for step, x in enumerate(dataset):\n",
    "    loss = training_step(x)\n",
    "    losses.append(float(loss))\n",
    "    if step % 100 == 0:\n",
    "        print(\"step:\", step, \"loss:\", sum(losses) / len(losses))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57600"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1800*32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dim = 784\n",
    "intermediate_dim = 64\n",
    "latent_dim = 32\n",
    "\n",
    "original_inputs = tf.keras.Input(shape = (original_dim, ), name = \"encoder_input\")\n",
    "x = layers.Dense(intermediate_dim, activation = \"relu\")(original_inputs)\n",
    "z_mean = layers.Dense(latent_dim, name = \"z_mean\")(x)\n",
    "z_log_var = layers.Dense(latent_dim, name = \"z_log_var\")(x)\n",
    "z = Sampling()((z_mean, z_log_var))\n",
    "encoder = tf.keras.Model(inputs=original_inputs, outputs = z, name = \"encoder\")\n",
    "\n",
    "latent_inputs = tf.keras.Input(shape = (latent_dim,), name = \"z_sampling\")\n",
    "x = layers.Dense(intermediate_dim, activation = \"relu\")(latent_inputs)\n",
    "outputs = layers.Dense(original_dim, activation=\"sigmoid\")(x)\n",
    "decoder = tf.keras.Model(inputs=latent_inputs, outputs = outputs, name = \"decoder\")\n",
    "\n",
    "outputs = decoder(z)\n",
    "vae = tf.keras.Model(inputs=original_inputs, outputs = outputs, name = \"vae\")\n",
    "\n",
    "kl_loss = -0.5 * tf.reduce_mean(z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1)\n",
    "vae.add_loss(kl_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0714\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f81c3d93fd0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "(x_train, _), _ = tf.keras.datasets.mnist.load_data()\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    x_train.reshape(60000, 784).astype(\"float32\") / 255\n",
    ")\n",
    "dataset = dataset.map(lambda x: (x, x))\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(32)\n",
    "vae.compile(optimizer, loss=loss_fn)\n",
    "vae.fit(dataset, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "input_dim = 784\n",
    "classes = 10\n",
    "\n",
    "outer_model = keras.Sequential(\n",
    "    [keras.layers.Dense(64, activation=tf.nn.relu), keras.layers.Dense(classes), ]\n",
    ")\n",
    "\n",
    "for layer in outer_model.layers:\n",
    "    layer.built = True\n",
    "\n",
    "num_weights_to_generate = (classes * 64 + classes) + (64 * input_dim + 64)\n",
    "\n",
    "inner_model = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Dense(16, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(num_weights_to_generate, activation=tf.nn.sigmoid)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 Loss: 3.106187582015991\n",
      "Step: 100 Loss: 2.329586129451152\n",
      "Step: 200 Loss: 2.018739344932102\n",
      "Step: 300 Loss: 1.8607817600085554\n",
      "Step: 400 Loss: 1.6927321594202092\n",
      "Step: 500 Loss: 1.659945924356461\n",
      "Step: 600 Loss: 1.6165531615731172\n",
      "Step: 700 Loss: 1.5848073587594667\n",
      "Step: 800 Loss: 1.5421692139055496\n",
      "Step: 900 Loss: 1.5195796266366954\n",
      "Step: 1000 Loss: 1.4668622419073138\n",
      "Step: 1100 Loss: 1.4206054525291298\n",
      "Step: 1200 Loss: 1.3794678447772861\n",
      "Step: 1300 Loss: 1.3303812805341877\n",
      "Step: 1400 Loss: 1.2965709597478559\n",
      "Step: 1500 Loss: 1.2763124426571206\n",
      "Step: 1600 Loss: 1.2495518829017889\n",
      "Step: 1700 Loss: 1.2231836422578808\n",
      "Step: 1800 Loss: 1.2097418638349406\n",
      "Step: 1900 Loss: 1.195731880292863\n",
      "Step: 2000 Loss: 1.1669522723871206\n",
      "Step: 2100 Loss: 1.1616932695890003\n",
      "Step: 2200 Loss: 1.1450523010648122\n",
      "Step: 2300 Loss: 1.1312813921083238\n",
      "Step: 2400 Loss: 1.1160248060201192\n",
      "Step: 2500 Loss: 1.109039789929457\n",
      "Step: 2600 Loss: 1.105918935104946\n",
      "Step: 2700 Loss: 1.095133691379496\n",
      "Step: 2800 Loss: 1.0811187687790131\n",
      "Step: 2900 Loss: 1.0709848276152312\n",
      "Step: 3000 Loss: 1.0560574197044363\n",
      "Step: 3100 Loss: 1.047922458679524\n",
      "Step: 3200 Loss: 1.0406606797642797\n",
      "Step: 3300 Loss: 1.0328443594794285\n",
      "Step: 3400 Loss: 1.0312937522889198\n",
      "Step: 3500 Loss: 1.0223192300468917\n",
      "Step: 3600 Loss: 1.0140984832876523\n",
      "Step: 3700 Loss: 1.0011612031794856\n",
      "Step: 3800 Loss: 0.9957349561687379\n",
      "Step: 3900 Loss: 0.9909850288898844\n",
      "Step: 4000 Loss: 0.980879941217779\n",
      "Step: 4100 Loss: 0.9739052104369981\n",
      "Step: 4200 Loss: 0.9664207378321772\n",
      "Step: 4300 Loss: 0.9650918681913871\n",
      "Step: 4400 Loss: 0.9615908521416172\n",
      "Step: 4500 Loss: 0.9539298464775924\n",
      "Step: 4600 Loss: 0.9548887084492345\n",
      "Step: 4700 Loss: 0.9501510556552857\n",
      "Step: 4800 Loss: 0.9491661147719852\n",
      "Step: 4900 Loss: 0.9426814031625566\n",
      "Step: 5000 Loss: 0.938191876946718\n",
      "Step: 5100 Loss: 0.9325849509352208\n",
      "Step: 5200 Loss: 0.926917569162326\n",
      "Step: 5300 Loss: 0.922077017528541\n",
      "Step: 5400 Loss: 0.9145845530863135\n",
      "Step: 5500 Loss: 0.9126011659129614\n",
      "Step: 5600 Loss: 0.907763004336628\n",
      "Step: 5700 Loss: 0.9028208475404466\n",
      "Step: 5800 Loss: 0.898215287022684\n",
      "Step: 5900 Loss: 0.8897353553084622\n",
      "Step: 6000 Loss: 0.8842866282871782\n",
      "Step: 6100 Loss: 0.8754810667951105\n",
      "Step: 6200 Loss: 0.8772008572381532\n",
      "Step: 6300 Loss: 0.8742730975427072\n",
      "Step: 6400 Loss: 0.8709578436761526\n",
      "Step: 6500 Loss: 0.8691064108305331\n",
      "Step: 6600 Loss: 0.8644915978139334\n",
      "Step: 6700 Loss: 0.8647080704848452\n",
      "Step: 6800 Loss: 0.8635922981941404\n",
      "Step: 6900 Loss: 0.8594128228054058\n",
      "Step: 7000 Loss: 0.8618021189133936\n",
      "Step: 7100 Loss: 0.8625770183402265\n",
      "Step: 7200 Loss: 0.8604576739549449\n",
      "Step: 7300 Loss: 0.8555940020280942\n",
      "Step: 7400 Loss: 0.8544437279786014\n",
      "Step: 7500 Loss: 0.8512423291281744\n",
      "Step: 7600 Loss: 0.8512915721994923\n",
      "Step: 7700 Loss: 0.8502585232714605\n",
      "Step: 7800 Loss: 0.8474556077257589\n",
      "Step: 7900 Loss: 0.8474724533640652\n",
      "Step: 8000 Loss: 0.846303449950714\n",
      "Step: 8100 Loss: 0.845663915852205\n",
      "Step: 8200 Loss: 0.8426319421207842\n",
      "Step: 8300 Loss: 0.8435500238763349\n",
      "Step: 8400 Loss: 0.8398198236937924\n",
      "Step: 8500 Loss: 0.8355577584534164\n",
      "Step: 8600 Loss: 0.8347805243713876\n",
      "Step: 8700 Loss: 0.831746865393769\n",
      "Step: 8800 Loss: 0.8317844049157855\n",
      "Step: 8900 Loss: 0.829513644886591\n",
      "Step: 9000 Loss: 0.8271806579424346\n",
      "Step: 9100 Loss: 0.82589398539488\n",
      "Step: 9200 Loss: 0.8237266919759825\n",
      "Step: 9300 Loss: 0.8222730011543425\n",
      "Step: 9400 Loss: 0.8187932992800167\n",
      "Step: 9500 Loss: 0.8180800165389098\n",
      "Step: 9600 Loss: 0.8170731278599502\n",
      "Step: 9700 Loss: 0.8122291653253507\n",
      "Step: 9800 Loss: 0.8091782759380842\n",
      "Step: 9900 Loss: 0.806595839810379\n",
      "Step: 10000 Loss: 0.8055982332743243\n",
      "Step: 10100 Loss: 0.8037380804504464\n",
      "Step: 10200 Loss: 0.8000806091231729\n",
      "Step: 10300 Loss: 0.7969368119170185\n",
      "Step: 10400 Loss: 0.7925689701762888\n",
      "Step: 10500 Loss: 0.7901995797409983\n",
      "Step: 10600 Loss: 0.7892698752903724\n",
      "Step: 10700 Loss: 0.7862094538566657\n",
      "Step: 10800 Loss: 0.788641363568341\n",
      "Step: 10900 Loss: 0.7881725615009977\n",
      "Step: 11000 Loss: 0.7866450992502336\n",
      "Step: 11100 Loss: 0.7831147368905297\n",
      "Step: 11200 Loss: 0.7824602441155784\n",
      "Step: 11300 Loss: 0.7803549376301475\n",
      "Step: 11400 Loss: 0.7797544864495464\n",
      "Step: 11500 Loss: 0.7798081218551137\n",
      "Step: 11600 Loss: 0.777915809427463\n",
      "Step: 11700 Loss: 0.7773286842365724\n",
      "Step: 11800 Loss: 0.7742839543863294\n",
      "Step: 11900 Loss: 0.7729760787921318\n",
      "Step: 12000 Loss: 0.7732508448141904\n",
      "Step: 12100 Loss: 0.7714440305653344\n",
      "Step: 12200 Loss: 0.7719017953018074\n",
      "Step: 12300 Loss: 0.7718825263801025\n",
      "Step: 12400 Loss: 0.7683040359362555\n",
      "Step: 12500 Loss: 0.7668398677389656\n",
      "Step: 12600 Loss: 0.7666669842005086\n",
      "Step: 12700 Loss: 0.7670336979259429\n",
      "Step: 12800 Loss: 0.7647261241104452\n",
      "Step: 12900 Loss: 0.7638728430464248\n",
      "Step: 13000 Loss: 0.7607049621321673\n",
      "Step: 13100 Loss: 0.7595205623809094\n",
      "Step: 13200 Loss: 0.7596966812049889\n",
      "Step: 13300 Loss: 0.7581275993195977\n",
      "Step: 13400 Loss: 0.7565949420138248\n",
      "Step: 13500 Loss: 0.7555909572683591\n",
      "Step: 13600 Loss: 0.7544044880400715\n",
      "Step: 13700 Loss: 0.753112102760304\n",
      "Step: 13800 Loss: 0.7533879512161582\n",
      "Step: 13900 Loss: 0.7515794236243425\n",
      "Step: 14000 Loss: 0.7500680978746548\n",
      "Step: 14100 Loss: 0.7496098327656585\n",
      "Step: 14200 Loss: 0.7504657652703289\n",
      "Step: 14300 Loss: 0.7493696924088167\n",
      "Step: 14400 Loss: 0.7479094438522981\n",
      "Step: 14500 Loss: 0.7481731846478702\n",
      "Step: 14600 Loss: 0.7474611204226007\n",
      "Step: 14700 Loss: 0.746954992026005\n",
      "Step: 14800 Loss: 0.7450911862427538\n",
      "Step: 14900 Loss: 0.7446617957644049\n",
      "Step: 15000 Loss: 0.7431207184888865\n",
      "Step: 15100 Loss: 0.7430785519502954\n",
      "Step: 15200 Loss: 0.7410142450870442\n",
      "Step: 15300 Loss: 0.7404289938700437\n",
      "Step: 15400 Loss: 0.7397709842553402\n",
      "Step: 15500 Loss: 0.7373451849168101\n",
      "Step: 15600 Loss: 0.7357610541586648\n",
      "Step: 15700 Loss: 0.7344530304135174\n",
      "Step: 15800 Loss: 0.7329426892586998\n",
      "Step: 15900 Loss: 0.7328176757683212\n",
      "Step: 16000 Loss: 0.7320584605412457\n",
      "Step: 16100 Loss: 0.7334633801815065\n",
      "Step: 16200 Loss: 0.7325063675955118\n",
      "Step: 16300 Loss: 0.7313404473626267\n",
      "Step: 16400 Loss: 0.7293703093760481\n",
      "Step: 16500 Loss: 0.7287015898082242\n",
      "Step: 16600 Loss: 0.727330676545319\n",
      "Step: 16700 Loss: 0.7262752616633777\n",
      "Step: 16800 Loss: 0.7245737756433228\n",
      "Step: 16900 Loss: 0.7253854872213653\n",
      "Step: 17000 Loss: 0.722496197641904\n",
      "Step: 17100 Loss: 0.7213588306598322\n",
      "Step: 17200 Loss: 0.7200428576365139\n",
      "Step: 17300 Loss: 0.7189730568294722\n",
      "Step: 17400 Loss: 0.7204584027349743\n",
      "Step: 17500 Loss: 0.7186794607310248\n",
      "Step: 17600 Loss: 0.7184468783235992\n",
      "Step: 17700 Loss: 0.7179200279149952\n",
      "Step: 17800 Loss: 0.7165509583819358\n",
      "Step: 17900 Loss: 0.7143481861092574\n",
      "Step: 18000 Loss: 0.7143235915415679\n",
      "Step: 18100 Loss: 0.7145639561104026\n",
      "Step: 18200 Loss: 0.7136723156484955\n",
      "Step: 18300 Loss: 0.7127228749359351\n",
      "Step: 18400 Loss: 0.7114145059202528\n",
      "Step: 18500 Loss: 0.7100491346619406\n",
      "Step: 18600 Loss: 0.708807321317055\n",
      "Step: 18700 Loss: 0.7094194545493273\n",
      "Step: 18800 Loss: 0.7079839301279588\n",
      "Step: 18900 Loss: 0.7058400922640421\n",
      "Step: 19000 Loss: 0.7042682177687639\n",
      "Step: 19100 Loss: 0.7033282384464803\n",
      "Step: 19200 Loss: 0.7014092337887445\n",
      "Step: 19300 Loss: 0.7019223180313512\n",
      "Step: 19400 Loss: 0.7007826396023344\n",
      "Step: 19500 Loss: 0.6993134505458156\n",
      "Step: 19600 Loss: 0.6978310757765307\n",
      "Step: 19700 Loss: 0.6961219328610094\n",
      "Step: 19800 Loss: 0.697100729093053\n",
      "Step: 19900 Loss: 0.6947496671687625\n",
      "Step: 20000 Loss: 0.6928867789979324\n",
      "Step: 20100 Loss: 0.690975896069734\n",
      "Step: 20200 Loss: 0.691359417782783\n",
      "Step: 20300 Loss: 0.689867455182416\n",
      "Step: 20400 Loss: 0.6900246498289575\n",
      "Step: 20500 Loss: 0.6894595788155229\n",
      "Step: 20600 Loss: 0.688816465631217\n",
      "Step: 20700 Loss: 0.6879352995470776\n",
      "Step: 20800 Loss: 0.6868189692184864\n",
      "Step: 20900 Loss: 0.6854557493966842\n",
      "Step: 21000 Loss: 0.6852029730836697\n",
      "Step: 21100 Loss: 0.685594519807221\n",
      "Step: 21200 Loss: 0.6836529160555711\n",
      "Step: 21300 Loss: 0.6835735991268665\n",
      "Step: 21400 Loss: 0.6829710662748356\n",
      "Step: 21500 Loss: 0.6815570300290542\n",
      "Step: 21600 Loss: 0.6812311830949975\n",
      "Step: 21700 Loss: 0.6811056025316503\n",
      "Step: 21800 Loss: 0.6804046599259514\n",
      "Step: 21900 Loss: 0.6796611164183822\n",
      "Step: 22000 Loss: 0.6790771110966324\n",
      "Step: 22100 Loss: 0.6783095011633232\n",
      "Step: 22200 Loss: 0.6769381474704382\n",
      "Step: 22300 Loss: 0.6759942879752459\n",
      "Step: 22400 Loss: 0.6747654201859595\n",
      "Step: 22500 Loss: 0.6743783762408355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 22600 Loss: 0.6738090926891418\n",
      "Step: 22700 Loss: 0.6731778062604573\n",
      "Step: 22800 Loss: 0.6740356664172105\n",
      "Step: 22900 Loss: 0.6724637007002229\n",
      "Step: 23000 Loss: 0.6712288585444788\n",
      "Step: 23100 Loss: 0.6700524873408495\n",
      "Step: 23200 Loss: 0.6688628845500976\n",
      "Step: 23300 Loss: 0.6683685356759446\n",
      "Step: 23400 Loss: 0.667677262552663\n",
      "Step: 23500 Loss: 0.6690476864466912\n",
      "Step: 23600 Loss: 0.668645532277996\n",
      "Step: 23700 Loss: 0.6687883072316537\n",
      "Step: 23800 Loss: 0.6679152346669608\n",
      "Step: 23900 Loss: 0.6675844639062294\n",
      "Step: 24000 Loss: 0.6671006059190772\n",
      "Step: 24100 Loss: 0.6662608155164917\n",
      "Step: 24200 Loss: 0.6659203600411859\n",
      "Step: 24300 Loss: 0.6655180255264141\n",
      "Step: 24400 Loss: 0.665540608494538\n",
      "Step: 24500 Loss: 0.6642075210007209\n",
      "Step: 24600 Loss: 0.6636307217964408\n",
      "Step: 24700 Loss: 0.6624666200016601\n",
      "Step: 24800 Loss: 0.6611687664982572\n",
      "Step: 24900 Loss: 0.6601427726244423\n",
      "Step: 25000 Loss: 0.6591171202529635\n",
      "Step: 25100 Loss: 0.6586777909497106\n",
      "Step: 25200 Loss: 0.6576330484826408\n",
      "Step: 25300 Loss: 0.6565082495901252\n",
      "Step: 25400 Loss: 0.654922563405751\n",
      "Step: 25500 Loss: 0.652869175978296\n",
      "Step: 25600 Loss: 0.6527504879096326\n",
      "Step: 25700 Loss: 0.6519910715359024\n",
      "Step: 25800 Loss: 0.652629285476734\n",
      "Step: 25900 Loss: 0.6516347733483175\n",
      "Step: 26000 Loss: 0.6511884265089999\n",
      "Step: 26100 Loss: 0.6511265466404846\n",
      "Step: 26200 Loss: 0.6505743825182594\n",
      "Step: 26300 Loss: 0.6506493873421471\n",
      "Step: 26400 Loss: 0.6505700284792868\n",
      "Step: 26500 Loss: 0.6493246405985744\n",
      "Step: 26600 Loss: 0.6493867136866464\n",
      "Step: 26700 Loss: 0.6484511641448488\n",
      "Step: 26800 Loss: 0.6480583662505491\n",
      "Step: 26900 Loss: 0.6482760037570563\n",
      "Step: 27000 Loss: 0.6475669107999664\n",
      "Step: 27100 Loss: 0.6466641069299713\n",
      "Step: 27200 Loss: 0.6461604753251495\n",
      "Step: 27300 Loss: 0.6448160476744137\n",
      "Step: 27400 Loss: 0.6447489281731611\n",
      "Step: 27500 Loss: 0.643291674327095\n",
      "Step: 27600 Loss: 0.6439455166645234\n",
      "Step: 27700 Loss: 0.644696796482509\n",
      "Step: 27800 Loss: 0.6440826768370217\n",
      "Step: 27900 Loss: 0.6437447602951889\n",
      "Step: 28000 Loss: 0.6424284386038707\n",
      "Step: 28100 Loss: 0.6414337450101435\n",
      "Step: 28200 Loss: 0.6398418893076188\n",
      "Step: 28300 Loss: 0.6388714412245821\n",
      "Step: 28400 Loss: 0.6379001654146804\n",
      "Step: 28500 Loss: 0.638378389691634\n",
      "Step: 28600 Loss: 0.6368672170583589\n",
      "Step: 28700 Loss: 0.6361738625787581\n",
      "Step: 28800 Loss: 0.6362952759004883\n",
      "Step: 28900 Loss: 0.6351241961218005\n",
      "Step: 29000 Loss: 0.6340759658821992\n",
      "Step: 29100 Loss: 0.6354484254357113\n",
      "Step: 29200 Loss: 0.6349896704195677\n",
      "Step: 29300 Loss: 0.6347865258679404\n",
      "Step: 29400 Loss: 0.6338829660741305\n",
      "Step: 29500 Loss: 0.6332088285465653\n",
      "Step: 29600 Loss: 0.6322750607077049\n",
      "Step: 29700 Loss: 0.6315853559111528\n",
      "Step: 29800 Loss: 0.6305786254759467\n",
      "Step: 29900 Loss: 0.6301379813469223\n",
      "Step: 30000 Loss: 0.629326980113916\n",
      "Step: 30100 Loss: 0.6293425008250687\n",
      "Step: 30200 Loss: 0.6287978562683626\n",
      "Step: 30300 Loss: 0.6274404841378345\n",
      "Step: 30400 Loss: 0.6270304437135779\n",
      "Step: 30500 Loss: 0.625976618463608\n",
      "Step: 30600 Loss: 0.6250741614844065\n",
      "Step: 30700 Loss: 0.6259234373622651\n",
      "Step: 30800 Loss: 0.6260331264701771\n",
      "Step: 30900 Loss: 0.6264065038222012\n",
      "Step: 31000 Loss: 0.6258468341067787\n",
      "Step: 31100 Loss: 0.6254837570804958\n",
      "Step: 31200 Loss: 0.6263787425465198\n",
      "Step: 31300 Loss: 0.6258185514306444\n",
      "Step: 31400 Loss: 0.6248278544387676\n",
      "Step: 31500 Loss: 0.6238070232662544\n",
      "Step: 31600 Loss: 0.624265563094047\n",
      "Step: 31700 Loss: 0.6230669286712202\n",
      "Step: 31800 Loss: 0.6222850785932809\n",
      "Step: 31900 Loss: 0.622797404260325\n",
      "Step: 32000 Loss: 0.6223693092514286\n",
      "Step: 32100 Loss: 0.6212569337115609\n",
      "Step: 32200 Loss: 0.6206786768173581\n",
      "Step: 32300 Loss: 0.6203966148654492\n",
      "Step: 32400 Loss: 0.6197903368135894\n",
      "Step: 32500 Loss: 0.6197044000752622\n",
      "Step: 32600 Loss: 0.6191553770387103\n",
      "Step: 32700 Loss: 0.6181201014554921\n",
      "Step: 32800 Loss: 0.6175304395444431\n",
      "Step: 32900 Loss: 0.6169056543296864\n",
      "Step: 33000 Loss: 0.6168689683660029\n",
      "Step: 33100 Loss: 0.6154419559175277\n",
      "Step: 33200 Loss: 0.6153782419226024\n",
      "Step: 33300 Loss: 0.6147017659342253\n",
      "Step: 33400 Loss: 0.6139049610002092\n",
      "Step: 33500 Loss: 0.6130562329735915\n",
      "Step: 33600 Loss: 0.6120112556538317\n",
      "Step: 33700 Loss: 0.6118133449639774\n",
      "Step: 33800 Loss: 0.611834416008683\n",
      "Step: 33900 Loss: 0.6105821807231417\n",
      "Step: 34000 Loss: 0.60990248685044\n",
      "Step: 34100 Loss: 0.6090122987516121\n",
      "Step: 34200 Loss: 0.6102472282611577\n",
      "Step: 34300 Loss: 0.6100631019372882\n",
      "Step: 34400 Loss: 0.6092480453985907\n",
      "Step: 34500 Loss: 0.6085693140233064\n",
      "Step: 34600 Loss: 0.6083001840869147\n",
      "Step: 34700 Loss: 0.6076380233817225\n",
      "Step: 34800 Loss: 0.6072409951837987\n",
      "Step: 34900 Loss: 0.6072111602428609\n",
      "Step: 35000 Loss: 0.6066225320738067\n",
      "Step: 35100 Loss: 0.6061328779099584\n",
      "Step: 35200 Loss: 0.6059037205944736\n",
      "Step: 35300 Loss: 0.6050469949207903\n",
      "Step: 35400 Loss: 0.6047056452992535\n",
      "Step: 35500 Loss: 0.6045809351414632\n",
      "Step: 35600 Loss: 0.6045273863667374\n",
      "Step: 35700 Loss: 0.6035434846610099\n",
      "Step: 35800 Loss: 0.6026233611474366\n",
      "Step: 35900 Loss: 0.6020528830614901\n",
      "Step: 36000 Loss: 0.6009118157931274\n",
      "Step: 36100 Loss: 0.6010970182098526\n",
      "Step: 36200 Loss: 0.601135132594826\n",
      "Step: 36300 Loss: 0.6006851842149264\n",
      "Step: 36400 Loss: 0.6004772155818223\n",
      "Step: 36500 Loss: 0.6005084844350872\n",
      "Step: 36600 Loss: 0.6007592812550935\n",
      "Step: 36700 Loss: 0.6007555886830872\n",
      "Step: 36800 Loss: 0.6007168594975949\n",
      "Step: 36900 Loss: 0.6007955008122213\n",
      "Step: 37000 Loss: 0.6004332025307476\n",
      "Step: 37100 Loss: 0.5996926163393164\n",
      "Step: 37200 Loss: 0.5997553126319909\n",
      "Step: 37300 Loss: 0.5992337002429494\n",
      "Step: 37400 Loss: 0.5992282576963125\n",
      "Step: 37500 Loss: 0.5987108583474544\n",
      "Step: 37600 Loss: 0.5991994381637352\n",
      "Step: 37700 Loss: 0.6005101229717298\n",
      "Step: 37800 Loss: 0.5999888275709093\n",
      "Step: 37900 Loss: 0.5992565949597558\n",
      "Step: 38000 Loss: 0.5986864907136353\n",
      "Step: 38100 Loss: 0.5985418449119096\n",
      "Step: 38200 Loss: 0.5977497329255361\n",
      "Step: 38300 Loss: 0.5967977977285245\n",
      "Step: 38400 Loss: 0.5962169601209986\n",
      "Step: 38500 Loss: 0.5958250354270352\n",
      "Step: 38600 Loss: 0.5948785288188096\n",
      "Step: 38700 Loss: 0.5944005400864776\n",
      "Step: 38800 Loss: 0.5937554644918994\n",
      "Step: 38900 Loss: 0.5928212347455428\n",
      "Step: 39000 Loss: 0.5927711094510468\n",
      "Step: 39100 Loss: 0.5921621922375842\n",
      "Step: 39200 Loss: 0.5919518052550625\n",
      "Step: 39300 Loss: 0.5915477640467477\n",
      "Step: 39400 Loss: 0.5909090112600475\n",
      "Step: 39500 Loss: 0.5904708918500444\n",
      "Step: 39600 Loss: 0.5903484567944797\n",
      "Step: 39700 Loss: 0.5895663873588669\n",
      "Step: 39800 Loss: 0.5887677635822876\n",
      "Step: 39900 Loss: 0.5883944459745502\n",
      "Step: 40000 Loss: 0.5879406110828953\n",
      "Step: 40100 Loss: 0.5867763815580259\n",
      "Step: 40200 Loss: 0.5857863335673481\n",
      "Step: 40300 Loss: 0.5853721842277365\n",
      "Step: 40400 Loss: 0.5861979255258669\n",
      "Step: 40500 Loss: 0.586885298111093\n",
      "Step: 40600 Loss: 0.5867731826334432\n",
      "Step: 40700 Loss: 0.5860249356082714\n",
      "Step: 40800 Loss: 0.5858111898483059\n",
      "Step: 40900 Loss: 0.5848160161677838\n",
      "Step: 41000 Loss: 0.5843527608756355\n",
      "Step: 41100 Loss: 0.5846118427602651\n",
      "Step: 41200 Loss: 0.5836473403261891\n",
      "Step: 41300 Loss: 0.5833618359592024\n",
      "Step: 41400 Loss: 0.5830332228876937\n",
      "Step: 41500 Loss: 0.582494157663647\n",
      "Step: 41600 Loss: 0.5819974284309098\n",
      "Step: 41700 Loss: 0.5814926899800555\n",
      "Step: 41800 Loss: 0.5810436350033935\n",
      "Step: 41900 Loss: 0.5804147695223703\n",
      "Step: 42000 Loss: 0.5809071212064588\n",
      "Step: 42100 Loss: 0.5807659680012849\n",
      "Step: 42200 Loss: 0.5801196549894247\n",
      "Step: 42300 Loss: 0.5798722524438018\n",
      "Step: 42400 Loss: 0.5797425995525637\n",
      "Step: 42500 Loss: 0.5789569022700755\n",
      "Step: 42600 Loss: 0.5789772296370168\n",
      "Step: 42700 Loss: 0.5791996806640716\n",
      "Step: 42800 Loss: 0.5786519377716255\n",
      "Step: 42900 Loss: 0.5778726848171881\n",
      "Step: 43000 Loss: 0.5770629320661051\n",
      "Step: 43100 Loss: 0.5764115907577567\n",
      "Step: 43200 Loss: 0.5760384945779009\n",
      "Step: 43300 Loss: 0.5756607037628192\n",
      "Step: 43400 Loss: 0.5764321683262956\n",
      "Step: 43500 Loss: 0.5759764590542211\n",
      "Step: 43600 Loss: 0.575367323408233\n",
      "Step: 43700 Loss: 0.5752427852296813\n",
      "Step: 43800 Loss: 0.5742984746866949\n",
      "Step: 43900 Loss: 0.5735767243319546\n",
      "Step: 44000 Loss: 0.5735116655451165\n",
      "Step: 44100 Loss: 0.5736206934381319\n",
      "Step: 44200 Loss: 0.5730993490490689\n",
      "Step: 44300 Loss: 0.5723907569943232\n",
      "Step: 44400 Loss: 0.5718213403908929\n",
      "Step: 44500 Loss: 0.5711654960067324\n",
      "Step: 44600 Loss: 0.570568662006099\n",
      "Step: 44700 Loss: 0.5702633721478115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 44800 Loss: 0.5702724872351568\n",
      "Step: 44900 Loss: 0.5698007984211281\n",
      "Step: 45000 Loss: 0.5696763269937449\n",
      "Step: 45100 Loss: 0.5696589972727357\n",
      "Step: 45200 Loss: 0.5695521961702927\n",
      "Step: 45300 Loss: 0.5693974505078573\n",
      "Step: 45400 Loss: 0.5691841045362773\n",
      "Step: 45500 Loss: 0.5686395642170001\n",
      "Step: 45600 Loss: 0.5681956086416007\n",
      "Step: 45700 Loss: 0.5675832020871933\n",
      "Step: 45800 Loss: 0.5679150579549092\n",
      "Step: 45900 Loss: 0.5676504391347579\n",
      "Step: 46000 Loss: 0.5671349417827459\n",
      "Step: 46100 Loss: 0.5667084162152586\n",
      "Step: 46200 Loss: 0.5670787228121708\n",
      "Step: 46300 Loss: 0.5671918484216262\n",
      "Step: 46400 Loss: 0.5667944552318642\n",
      "Step: 46500 Loss: 0.5664136736830282\n",
      "Step: 46600 Loss: 0.5659280183528874\n",
      "Step: 46700 Loss: 0.5658875210141514\n",
      "Step: 46800 Loss: 0.5653765335760951\n",
      "Step: 46900 Loss: 0.5655537515132901\n",
      "Step: 47000 Loss: 0.5650273041516409\n",
      "Step: 47100 Loss: 0.5649597433358065\n",
      "Step: 47200 Loss: 0.5647823413184038\n",
      "Step: 47300 Loss: 0.5648534882403522\n",
      "Step: 47400 Loss: 0.5644253013605042\n",
      "Step: 47500 Loss: 0.5644588133756488\n",
      "Step: 47600 Loss: 0.5645334310696538\n",
      "Step: 47700 Loss: 0.564204828387281\n",
      "Step: 47800 Loss: 0.5637510582256521\n",
      "Step: 47900 Loss: 0.5631701954035605\n",
      "Step: 48000 Loss: 0.5631871653480506\n",
      "Step: 48100 Loss: 0.562484483329276\n",
      "Step: 48200 Loss: 0.5621193248016805\n",
      "Step: 48300 Loss: 0.5619329596756997\n",
      "Step: 48400 Loss: 0.5619222417096803\n",
      "Step: 48500 Loss: 0.5613752271360319\n",
      "Step: 48600 Loss: 0.5613949688023054\n",
      "Step: 48700 Loss: 0.5611990079748485\n",
      "Step: 48800 Loss: 0.5608919534818478\n",
      "Step: 48900 Loss: 0.5605614349560815\n",
      "Step: 49000 Loss: 0.5606163111366154\n",
      "Step: 49100 Loss: 0.5607626030967886\n",
      "Step: 49200 Loss: 0.5603140705356292\n",
      "Step: 49300 Loss: 0.5601188628418389\n",
      "Step: 49400 Loss: 0.5598727510428242\n",
      "Step: 49500 Loss: 0.5600804195989557\n",
      "Step: 49600 Loss: 0.5596178691313998\n",
      "Step: 49700 Loss: 0.5591647295077264\n",
      "Step: 49800 Loss: 0.5594763062989733\n",
      "Step: 49900 Loss: 0.5597546793330588\n",
      "Step: 50000 Loss: 0.5596894493941965\n",
      "Step: 50100 Loss: 0.559078449118073\n",
      "Step: 50200 Loss: 0.5585280197078774\n",
      "Step: 50300 Loss: 0.5581272278557566\n",
      "Step: 50400 Loss: 0.5582415056281552\n",
      "Step: 50500 Loss: 0.5578476273969278\n",
      "Step: 50600 Loss: 0.5573159232087707\n",
      "Step: 50700 Loss: 0.5566009917303887\n",
      "Step: 50800 Loss: 0.5559910583696415\n",
      "Step: 50900 Loss: 0.5552797964116782\n",
      "Step: 51000 Loss: 0.5550068935990676\n",
      "Step: 51100 Loss: 0.5548866878815466\n",
      "Step: 51200 Loss: 0.5551321433573979\n",
      "Step: 51300 Loss: 0.5549552242309935\n",
      "Step: 51400 Loss: 0.5544176939893752\n",
      "Step: 51500 Loss: 0.5538952976381982\n",
      "Step: 51600 Loss: 0.5539495238231273\n",
      "Step: 51700 Loss: 0.5531657276693474\n",
      "Step: 51800 Loss: 0.5531122735723961\n",
      "Step: 51900 Loss: 0.5526028032461839\n",
      "Step: 52000 Loss: 0.552541813300843\n",
      "Step: 52100 Loss: 0.5522374034334965\n",
      "Step: 52200 Loss: 0.5518044256712332\n",
      "Step: 52300 Loss: 0.5514284393972045\n",
      "Step: 52400 Loss: 0.5511631596867143\n",
      "Step: 52500 Loss: 0.5510085159610482\n",
      "Step: 52600 Loss: 0.5510792777695898\n",
      "Step: 52700 Loss: 0.5507292240101258\n",
      "Step: 52800 Loss: 0.5501841103395728\n",
      "Step: 52900 Loss: 0.5503087280512895\n",
      "Step: 53000 Loss: 0.5501396641890908\n",
      "Step: 53100 Loss: 0.5500948371467255\n",
      "Step: 53200 Loss: 0.5496283724825394\n",
      "Step: 53300 Loss: 0.5493654591001026\n",
      "Step: 53400 Loss: 0.5490058480067547\n",
      "Step: 53500 Loss: 0.548423162768743\n",
      "Step: 53600 Loss: 0.5478981348518092\n",
      "Step: 53700 Loss: 0.5472630621279886\n",
      "Step: 53800 Loss: 0.5470591192578461\n",
      "Step: 53900 Loss: 0.5468142862042181\n",
      "Step: 54000 Loss: 0.5465379688132402\n",
      "Step: 54100 Loss: 0.545969523621794\n",
      "Step: 54200 Loss: 0.545521089727299\n",
      "Step: 54300 Loss: 0.5455384882362806\n",
      "Step: 54400 Loss: 0.5453913504864785\n",
      "Step: 54500 Loss: 0.5448454702023249\n",
      "Step: 54600 Loss: 0.5447685364210075\n",
      "Step: 54700 Loss: 0.5441026068507434\n",
      "Step: 54800 Loss: 0.54416707529481\n",
      "Step: 54900 Loss: 0.5437506088340243\n",
      "Step: 55000 Loss: 0.5437796699822356\n",
      "Step: 55100 Loss: 0.5433180563533035\n",
      "Step: 55200 Loss: 0.5426741886005874\n",
      "Step: 55300 Loss: 0.5421731884585129\n",
      "Step: 55400 Loss: 0.5417435299490073\n",
      "Step: 55500 Loss: 0.541194144521264\n",
      "Step: 55600 Loss: 0.5406985389433333\n",
      "Step: 55700 Loss: 0.5402193402252906\n",
      "Step: 55800 Loss: 0.5397557111674888\n",
      "Step: 55900 Loss: 0.5396389374371692\n",
      "Step: 56000 Loss: 0.5391395893460649\n",
      "Step: 56100 Loss: 0.5384639540318589\n",
      "Step: 56200 Loss: 0.5385981256330633\n",
      "Step: 56300 Loss: 0.5380211654386587\n",
      "Step: 56400 Loss: 0.5374447195373455\n",
      "Step: 56500 Loss: 0.5370298952142665\n",
      "Step: 56600 Loss: 0.5364839583772856\n",
      "Step: 56700 Loss: 0.5364917033235362\n",
      "Step: 56800 Loss: 0.5360461821380667\n",
      "Step: 56900 Loss: 0.5356419641293519\n",
      "Step: 57000 Loss: 0.5352153525278351\n",
      "Step: 57100 Loss: 0.5352228687664389\n",
      "Step: 57200 Loss: 0.5349693762376764\n",
      "Step: 57300 Loss: 0.5346161626905531\n",
      "Step: 57400 Loss: 0.5340597602672039\n",
      "Step: 57500 Loss: 0.5335723381202852\n",
      "Step: 57600 Loss: 0.533100121159461\n",
      "Step: 57700 Loss: 0.5323682907073182\n",
      "Step: 57800 Loss: 0.5319075414245215\n",
      "Step: 57900 Loss: 0.5317383189221807\n",
      "Step: 58000 Loss: 0.5312076526063025\n",
      "Step: 58100 Loss: 0.530668743593706\n",
      "Step: 58200 Loss: 0.5303720238404072\n",
      "Step: 58300 Loss: 0.5298679507464795\n",
      "Step: 58400 Loss: 0.5293007824958316\n",
      "Step: 58500 Loss: 0.5289069479155476\n",
      "Step: 58600 Loss: 0.5282215362731262\n",
      "Step: 58700 Loss: 0.5279151487644368\n",
      "Step: 58800 Loss: 0.5275715398683539\n",
      "Step: 58900 Loss: 0.5276674174745991\n",
      "Step: 59000 Loss: 0.5271220314412001\n",
      "Step: 59100 Loss: 0.5265914823083189\n",
      "Step: 59200 Loss: 0.5260594480505233\n",
      "Step: 59300 Loss: 0.5258126223621952\n",
      "Step: 59400 Loss: 0.5250564128125442\n",
      "Step: 59500 Loss: 0.5244146564202536\n",
      "Step: 59600 Loss: 0.5245120009295225\n",
      "Step: 59700 Loss: 0.5238307942182004\n",
      "Step: 59800 Loss: 0.5238490513470103\n",
      "Step: 59900 Loss: 0.5235458661782579\n"
     ]
    }
   ],
   "source": [
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "\n",
    "# Prepare a dataset.\n",
    "(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train.reshape(60000, 784).astype(\"float32\") / 255, y_train)\n",
    ")\n",
    "\n",
    "# We'll use a batch size of 1 for this experiment.\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(1)\n",
    "\n",
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        weights_pred = inner_model(x)\n",
    "        \n",
    "        start_index = 0\n",
    "        w0_shape = (input_dim, 64)\n",
    "        w0_coeffs = weights_pred[:, start_index : start_index + np.prod(w0_shape)]\n",
    "        w0 = tf.reshape(w0_coeffs, w0_shape)\n",
    "        start_index += np.prod(w0_shape)\n",
    "        # Layer 0 bias.\n",
    "        b0_shape = (64,)\n",
    "        b0_coeffs = weights_pred[:, start_index : start_index + np.prod(b0_shape)]\n",
    "        b0 = tf.reshape(b0_coeffs, b0_shape)\n",
    "        start_index += np.prod(b0_shape)\n",
    "        # Layer 1 kernel.\n",
    "        w1_shape = (64, classes)\n",
    "        w1_coeffs = weights_pred[:, start_index : start_index + np.prod(w1_shape)]\n",
    "        w1 = tf.reshape(w1_coeffs, w1_shape)\n",
    "        start_index += np.prod(w1_shape)\n",
    "        # Layer 1 bias.\n",
    "        b1_shape = (classes,)\n",
    "        b1_coeffs = weights_pred[:, start_index : start_index + np.prod(b1_shape)]\n",
    "        b1 = tf.reshape(b1_coeffs, b1_shape)\n",
    "        start_index += np.prod(b1_shape)\n",
    "        \n",
    "        outer_model.layers[0].kernel = w0\n",
    "        outer_model.layers[0].bias = b0\n",
    "        outer_model.layers[1].kernel = w1\n",
    "        outer_model.layers[1].bias = b1\n",
    "        \n",
    "        preds = outer_model(x)\n",
    "        loss = loss_fn(y, preds)\n",
    "    \n",
    "    grads = tape.gradient(loss, inner_model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, inner_model.trainable_weights))\n",
    "    return loss\n",
    "\n",
    "losses = []  # Keep track of the losses over time.\n",
    "for step, (x, y) in enumerate(dataset):\n",
    "    loss = train_step(x, y)\n",
    "\n",
    "    # Logging.\n",
    "    losses.append(float(loss))\n",
    "    if step % 100 == 0:\n",
    "        print(\"Step:\", step, \"Loss:\", sum(losses) / len(losses))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37 (base)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
